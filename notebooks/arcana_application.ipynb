{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Pre-Implemented Analyses\n",
    "\n",
    "An *Analysis* class can be applied to a dataset in a flexible manner, such as the parameterisation how and where the data is stored, which derivatives are required, and the computing environment in which to generate the derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Analysis Class\n",
    "\n",
    "We will start by importing a predefined Analysis class from `example.analysis`, which performs the same analysis as the workflow in the [Workflows Notebook](basic_workflow.ipynb). We print the \"menu\", the list of inputs, derivatives and parameters objects of this class can receive/derive, using the `static_menu` class method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "example.analysis.BasicBrainAnalysis Menu \n",
      "-----------------------------------------\n",
      "\n",
      "Inputs:\n",
      "    magnitude : nifti_gz\n",
      "        A magnitude image (e.g. T1w, T2w, etc..)\n",
      "\n",
      "Outputs:\n",
      "    brain : nifti_gz\n",
      "        Skull-stripped magnitude image\n",
      "    smooth : nifti_gz\n",
      "        Smoothed magnitude image\n",
      "    smooth_masked : nifti_gz\n",
      "        Smoothed and masked magnitude image\n",
      "\n",
      "Parameters:\n",
      "    smoothing_fwhm : float\n",
      "        The full-width-half-maxium radius of the smoothing kernel\n"
     ]
    }
   ],
   "source": [
    "from example.analysis import BasicBrainAnalysis\n",
    "print(BasicBrainAnalysis.static_menu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the \"full\" menu pass the 'full' flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "example.analysis.BasicBrainAnalysis Menu \n",
      "-----------------------------------------\n",
      "\n",
      "Inputs:\n",
      "    magnitude : nifti_gz\n",
      "        A magnitude image (e.g. T1w, T2w, etc..)\n",
      "\n",
      "Intermediate:\n",
      "    brain_mask : nifti_gz\n",
      "        Brain mask used for skull-stripping\n",
      "\n",
      "Outputs:\n",
      "    brain : nifti_gz\n",
      "        Skull-stripped magnitude image\n",
      "    smooth : nifti_gz\n",
      "        Smoothed magnitude image\n",
      "    smooth_masked : nifti_gz\n",
      "        Smoothed and masked magnitude image\n",
      "\n",
      "Parameters:\n",
      "    smoothing_fwhm : float\n",
      "        The full-width-half-maxium radius of the smoothing kernel\n"
     ]
    }
   ],
   "source": [
    "print(BasicBrainAnalysis.static_menu(full=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Dataset to Analyse\n",
    "\n",
    "Arcana implicitly handles a lot of the menial tasks involved with data input/outputs such as file format conversions and inserting/retrieving data from a repository service (e.g. XNAT). To specify where your data is you need to create a Dataset object.\n",
    "\n",
    "### Datasets in Directories on Local System\n",
    "\n",
    "The simplest form of dataset object is just a directory on (or mounted on) your local file system. The structure of this directory depends on its \"depth\", i.e. whether it has multiple subjects and visits in it or not.\n",
    "\n",
    "#### Depth: 0\n",
    "\n",
    "Typically, just used for prototyping, but you can define a dataset for a single subject by just storing all the data within a single directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/sample-datasets/depth0\n",
      "├── sub-01_ses-test_T1w.nii.gz\n",
      "├── sub-01_ses-test_T1w_bet.nii.gz\n",
      "├── sub-01_ses-test_dwi.nii.gz\n",
      "├── sub-01_ses-test_task-covertverbgeneration_bold.nii.gz\n",
      "├── sub-01_ses-test_task-fingerfootlips_bold.nii.gz\n",
      "├── sub-01_ses-test_task-linebisection_bold.nii.gz\n",
      "├── sub-01_ses-test_task-overtverbgeneration_bold.nii.gz\n",
      "└── sub-01_ses-test_task-overtwordrepetition_bold.nii.gz\n",
      "\n",
      "0 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Create a dataset for a single session in a flat directory. We will copy data from the BIDS formatted ds000114\n",
    "SAMPLE_DSET=output/sample-datasets/depth0\n",
    "mkdir -p $SAMPLE_DSET\n",
    "find data/ds000114/sub-01/ses-test -name '*.nii.gz' | xargs -I% cp -f % $SAMPLE_DSET/\n",
    "tree $SAMPLE_DSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(name='/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth0', depth=0, repository=LocalFileSystemRepo())\n"
     ]
    }
   ],
   "source": [
    "from arcana import Dataset\n",
    "dset0 = Dataset('output/sample-datasets/depth0')\n",
    "print(dset0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `depth` of this dataset is `0`. This means that there aren't any sub-directories for separate subjects or visits in it. However, all datasets in Arcana have an implicit depth of 2 (although future versions may relax this restriction) so we can see that the single \"session\" (a single visit of a subject) is assigned default subject and visit IDs of 'SUBJECT' and 'VISIT' respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects: ['SUBJECT']\n",
      "visits: ['VISIT']\n"
     ]
    }
   ],
   "source": [
    "print('subjects:', list(dset0.subject_ids))\n",
    "print('visits:', list(dset0.visit_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depth: 1\n",
    "\n",
    "For a multi-subject dataset we can add sub-directories for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/sample-datasets/depth1\n",
      "├── sub1\n",
      "│   ├── sub-01_ses-test_T1w.nii.gz\n",
      "│   ├── sub-01_ses-test_T1w_bet.nii.gz\n",
      "│   ├── sub-01_ses-test_dwi.nii.gz\n",
      "│   ├── sub-01_ses-test_task-covertverbgeneration_bold.nii.gz\n",
      "│   ├── sub-01_ses-test_task-fingerfootlips_bold.nii.gz\n",
      "│   ├── sub-01_ses-test_task-linebisection_bold.nii.gz\n",
      "│   ├── sub-01_ses-test_task-overtverbgeneration_bold.nii.gz\n",
      "│   └── sub-01_ses-test_task-overtwordrepetition_bold.nii.gz\n",
      "├── sub2\n",
      "│   ├── sub-02_ses-test_T1w.nii.gz\n",
      "│   ├── sub-02_ses-test_dwi.nii.gz\n",
      "│   ├── sub-02_ses-test_task-covertverbgeneration_bold.nii.gz\n",
      "│   ├── sub-02_ses-test_task-fingerfootlips_bold.nii.gz\n",
      "│   ├── sub-02_ses-test_task-linebisection_bold.nii.gz\n",
      "│   ├── sub-02_ses-test_task-overtverbgeneration_bold.nii.gz\n",
      "│   └── sub-02_ses-test_task-overtwordrepetition_bold.nii.gz\n",
      "└── sub3\n",
      "    ├── sub-03_ses-test_T1w.nii.gz\n",
      "    ├── sub-03_ses-test_dwi.nii.gz\n",
      "    ├── sub-03_ses-test_task-covertverbgeneration_bold.nii.gz\n",
      "    ├── sub-03_ses-test_task-fingerfootlips_bold.nii.gz\n",
      "    ├── sub-03_ses-test_task-linebisection_bold.nii.gz\n",
      "    ├── sub-03_ses-test_task-overtverbgeneration_bold.nii.gz\n",
      "    └── sub-03_ses-test_task-overtwordrepetition_bold.nii.gz\n",
      "\n",
      "3 directories, 22 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Create a dataset for a multiple subjects in separate sub-directories by copying data from the BIDS formatted ds000114\n",
    "SAMPLE_DSET=output/sample-datasets/depth1\n",
    "mkdir -p $SAMPLE_DSET/sub1 $SAMPLE_DSET/sub2  $SAMPLE_DSET/sub3\n",
    "find data/ds000114/sub-01/ses-test -name '*.nii.gz' | xargs -I% cp -f % $SAMPLE_DSET/sub1\n",
    "find data/ds000114/sub-02/ses-test -name '*.nii.gz' | xargs -I% cp -f % $SAMPLE_DSET/sub2\n",
    "find data/ds000114/sub-03/ses-test -name '*.nii.gz' | xargs -I% cp -f % $SAMPLE_DSET/sub3\n",
    "tree $SAMPLE_DSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(name='/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth1', depth=1, repository=LocalFileSystemRepo())\n",
      "subjects: ['sub1', 'sub2', 'sub3']\n",
      "visits: ['VISIT']\n"
     ]
    }
   ],
   "source": [
    "dset1 = Dataset('output/sample-datasets/depth1', depth=1)\n",
    "print(dset1)\n",
    "print('subjects:', list(dset1.subject_ids))\n",
    "print('visits:', list(dset1.visit_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that we need to explicitly provide the depth of `1` otherwise Arcana will interpret our 'sub1', 'sub2' and 'sub3' as filesets.\n",
    "\n",
    "#### Depth: 2\n",
    "\n",
    "For a dataset with multiple visits per subject we use a `depth == 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/sample-datasets/depth2\n",
      "├── sub1\n",
      "│   ├── retest\n",
      "│   │   ├── sub-01_ses-retest_T1w.nii.gz\n",
      "│   │   ├── sub-01_ses-retest_dwi.nii.gz\n",
      "│   │   ├── sub-01_ses-retest_task-covertverbgeneration_bold.nii.gz\n",
      "│   │   ├── sub-01_ses-retest_task-fingerfootlips_bold.nii.gz\n",
      "│   │   ├── sub-01_ses-retest_task-linebisection_bold.nii.gz\n",
      "│   │   ├── sub-01_ses-retest_task-overtverbgeneration_bold.nii.gz\n",
      "│   │   └── sub-01_ses-retest_task-overtwordrepetition_bold.nii.gz\n",
      "│   └── test\n",
      "│       ├── sub-01_ses-test_T1w.nii.gz\n",
      "│       ├── sub-01_ses-test_T1w_bet.nii.gz\n",
      "│       ├── sub-01_ses-test_dwi.nii.gz\n",
      "│       ├── sub-01_ses-test_task-covertverbgeneration_bold.nii.gz\n",
      "│       ├── sub-01_ses-test_task-fingerfootlips_bold.nii.gz\n",
      "│       ├── sub-01_ses-test_task-linebisection_bold.nii.gz\n",
      "│       ├── sub-01_ses-test_task-overtverbgeneration_bold.nii.gz\n",
      "│       └── sub-01_ses-test_task-overtwordrepetition_bold.nii.gz\n",
      "├── sub2\n",
      "│   ├── retest\n",
      "│   │   ├── sub-02_ses-retest_T1w.nii.gz\n",
      "│   │   ├── sub-02_ses-retest_dwi.nii.gz\n",
      "│   │   ├── sub-02_ses-retest_task-covertverbgeneration_bold.nii.gz\n",
      "│   │   ├── sub-02_ses-retest_task-fingerfootlips_bold.nii.gz\n",
      "│   │   ├── sub-02_ses-retest_task-linebisection_bold.nii.gz\n",
      "│   │   ├── sub-02_ses-retest_task-overtverbgeneration_bold.nii.gz\n",
      "│   │   └── sub-02_ses-retest_task-overtwordrepetition_bold.nii.gz\n",
      "│   └── test\n",
      "│       ├── sub-02_ses-test_T1w.nii.gz\n",
      "│       ├── sub-02_ses-test_dwi.nii.gz\n",
      "│       ├── sub-02_ses-test_task-covertverbgeneration_bold.nii.gz\n",
      "│       ├── sub-02_ses-test_task-fingerfootlips_bold.nii.gz\n",
      "│       ├── sub-02_ses-test_task-linebisection_bold.nii.gz\n",
      "│       ├── sub-02_ses-test_task-overtverbgeneration_bold.nii.gz\n",
      "│       └── sub-02_ses-test_task-overtwordrepetition_bold.nii.gz\n",
      "└── sub3\n",
      "    ├── retest\n",
      "    │   ├── sub-03_ses-retest_T1w.nii.gz\n",
      "    │   ├── sub-03_ses-retest_dwi.nii.gz\n",
      "    │   ├── sub-03_ses-retest_task-covertverbgeneration_bold.nii.gz\n",
      "    │   ├── sub-03_ses-retest_task-fingerfootlips_bold.nii.gz\n",
      "    │   ├── sub-03_ses-retest_task-linebisection_bold.nii.gz\n",
      "    │   ├── sub-03_ses-retest_task-overtverbgeneration_bold.nii.gz\n",
      "    │   └── sub-03_ses-retest_task-overtwordrepetition_bold.nii.gz\n",
      "    └── test\n",
      "        ├── sub-03_ses-test_T1w.nii.gz\n",
      "        ├── sub-03_ses-test_dwi.nii.gz\n",
      "        ├── sub-03_ses-test_task-covertverbgeneration_bold.nii.gz\n",
      "        ├── sub-03_ses-test_task-fingerfootlips_bold.nii.gz\n",
      "        ├── sub-03_ses-test_task-linebisection_bold.nii.gz\n",
      "        ├── sub-03_ses-test_task-overtverbgeneration_bold.nii.gz\n",
      "        └── sub-03_ses-test_task-overtwordrepetition_bold.nii.gz\n",
      "\n",
      "9 directories, 43 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Create a dataset for a multiple subjects in separate sub-directories by copying data from the BIDS formatted ds000114\n",
    "SAMPLE_DSET=output/sample-datasets/depth2\n",
    "mkdir -p $SAMPLE_DSET/sub1/test $SAMPLE_DSET/sub1/retest $SAMPLE_DSET/sub2/test $SAMPLE_DSET/sub2/test \\\n",
    "         $SAMPLE_DSET/sub2/retest $SAMPLE_DSET/sub3/test $SAMPLE_DSET/sub3/retest\n",
    "find data/ds000114/sub-01/ses-test -name '*.nii.gz' | xargs -I% cp -f % $SAMPLE_DSET/sub1/test\n",
    "find data/ds000114/sub-02/ses-test -name '*.nii.gz' | xargs -I% cp -f % $SAMPLE_DSET/sub2/test\n",
    "find data/ds000114/sub-03/ses-test -name '*.nii.gz' | xargs -I% cp -f % $SAMPLE_DSET/sub3/test\n",
    "find data/ds000114/sub-01/ses-retest -name '*.nii.gz' | xargs -I% cp -f % $SAMPLE_DSET/sub1/retest\n",
    "find data/ds000114/sub-02/ses-retest -name '*.nii.gz' | xargs -I% cp -f % $SAMPLE_DSET/sub2/retest\n",
    "find data/ds000114/sub-03/ses-retest -name '*.nii.gz' | xargs -I% cp -f % $SAMPLE_DSET/sub3/retest\n",
    "tree $SAMPLE_DSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(name='/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth2', depth=2, repository=LocalFileSystemRepo())\n",
      "subjects: ['sub1', 'sub2', 'sub3']\n",
      "visits: ['retest', 'test']\n"
     ]
    }
   ],
   "source": [
    "dset2 = Dataset('output/sample-datasets/depth2', depth=2)\n",
    "print(dset2)\n",
    "print('subjects:', list(dset2.subject_ids))\n",
    "print('visits:', list(dset2.visit_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, just say the `retest` session of `Subject 3` was corrupted we could exclude it from the analysis by either dropping `Subject 3` or `retest` from the dataset by filtering the IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(name='/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth2', depth=2, repository=LocalFileSystemRepo())\n",
      "subjects: ['sub1', 'sub2']\n",
      "visits: ['retest', 'test']\n"
     ]
    }
   ],
   "source": [
    "dset2_filter_subs = Dataset('output/sample-datasets/depth2', depth=2, subject_ids=['sub1', 'sub2'])\n",
    "print(dset2_filter_subs)\n",
    "print('subjects:', list(dset2_filter_subs.subject_ids))\n",
    "print('visits:', list(dset2_filter_subs.visit_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter the visits used in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(name='/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth2', depth=2, repository=LocalFileSystemRepo())\n",
      "subjects: ['sub1', 'sub2', 'sub3']\n",
      "visits: ['test']\n"
     ]
    }
   ],
   "source": [
    "dset2_filter_vis = Dataset('output/sample-datasets/depth2', depth=2, visit_ids=['test'])\n",
    "print(dset2_filter_vis)\n",
    "print('subjects:', list(dset2_filter_vis.subject_ids))\n",
    "print('visits:', list(dset2_filter_vis.visit_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to filter both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(name='/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth2', depth=2, repository=LocalFileSystemRepo())\n",
      "subjects: ['sub1', 'sub2']\n",
      "visits: ['test']\n"
     ]
    }
   ],
   "source": [
    "dset2_filter_both = Dataset('output/sample-datasets/depth2', depth=2, subject_ids=['sub1', 'sub2'], visit_ids=['test'])\n",
    "print(dset2_filter_both)\n",
    "print('subjects:', list(dset2_filter_both.subject_ids))\n",
    "print('visits:', list(dset2_filter_both.visit_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets on XNAT\n",
    "\n",
    "In addition to data stored on your local file system, Arcana transparently handles all the interactions with datasets stored in an XNAT repository.\n",
    "\n",
    "To test this we will use a public project set up on Monash's public XNAT instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XnatRepo(server=http://xnat.monash.edu, cache_dir=/Users/tclose/xnat-cache)\n",
      "Dataset(name='MISC0002', depth=2, repository=XnatRepo(server=http://xnat.monash.edu, cache_dir=/Users/tclose/xnat-cache))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Detected a redirect from http://xnat.monash.edu to https://xnat.monash.edu, using https://xnat.monash.edu from now on\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects: ['sub01', 'sub02', 'sub03']\n",
      "visits: ['retest', 'test']\n"
     ]
    }
   ],
   "source": [
    "import os.path as op\n",
    "from arcana import XnatRepo\n",
    "xnat_repo = XnatRepo(server='https://xnat.monash.edu', cache_dir=op.expanduser('~/xnat-cache'))\n",
    "print(xnat_repo)\n",
    "xnat_dataset = xnat_repo.dataset('MISC0002')  # This is the ID of the project on MXNAT\n",
    "print(xnat_dataset)\n",
    "print('subjects:', list(xnat_dataset.subject_ids))\n",
    "print('visits:', list(xnat_dataset.visit_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you have a look at the 'MISC0002' project on https://xnat.monash.edu.au you will notice that subjects and sessions are labelled according to the conventions used at MBI, i.e. PROJECTID_SUBJECTID and PROJECTID_SUBJECTID_VISITID for subject and session IDs, respectively. This is a current limitation of Arcana although it should be relaxed in the next month or so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Other Repository/Dataset Types\n",
    "\n",
    "At this stage XNAT is the only data repository platform supported by Arcana. However, care has been taken to modularise the code as much as possible so it should be fairly straightforward to implement support for other platforms (e.g. Loris, DaRIS, MyTaRDIS) as long as they have a REST API (or equivalent) that enables you to list, get and put data. See the base repository class `arcana.repository.base.Repository` for details on the six abstract methods that need to be overriden. \n",
    "\n",
    "There used to be a DaRIS module in early versions of Arcana, which could be ressurected without too much effort if you have a DaRIS instance to test against.\n",
    "\n",
    "`Banana` also adds support for [BIDS](https://bids.neuroimaging.io) via the `BidsDataset`. The BidsDataset objects are able to parse BIDS specific tree structure and insert derivatives at in the `derivatives` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Software Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
