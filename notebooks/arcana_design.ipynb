{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Analysis Classes\n",
    "\n",
    "We will now learn how to create your own Analysis class by going through the implementation of the `BasicBrainAnalysis` class from the \"Applying Analysis Classes\" notebook.\n",
    "\n",
    "There are three key components of an Analysis class:\n",
    "\n",
    "* Data specification\n",
    "* Parameter specification\n",
    "* Pipeline constructor methods\n",
    "\n",
    "The data specification defines all inputs, outputs and intermediate derivatives. The parameter specification specifies the free (meta) parameters that can be used to customise the analysis. Pipeline constructor methods are just regular methods of the class that return a `Pipeline` that generates one or more derivatives.\n",
    "\n",
    "## Base and Meta-classes and Inheritance\n",
    "\n",
    "Every Analysis class should (at least indirectly) inherit from the `arcana.Analysis` base class. This contains methods to perform a lot of the \"magic\" that Arcana does. \n",
    "\n",
    "Arcana is designed to utilise inheritance of Analysis classes, but since Analysis classes specify a number of class attributes (i.e. for the data and parameter specifications) all Analysis classes need to be constructed by a special \"meta-class\", `arcana.AnalysisMetaClass`. However, you don't need to understand what is going on behind the scenes (or what a meta-class is even), just simply define your class like this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcana import Analysis, AnalysisMetaClass\n",
    "\n",
    "class MyBasicBrainAnalysis(Analysis, metaclass=AnalysisMetaClass):\n",
    "    \n",
    "    # Stuff goes here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data specification\n",
    "\n",
    "The data specification is the place to start designing an Analysis class. As the name suggests, it specifies all inputs, outputs and intermediate derivatives of the class via a list of \"data-spec\" objects:\n",
    "\n",
    "* `FilesetSpec` (intermediate file-set derivatives)\n",
    "* `InputFilesetSpec`\n",
    "* `OutputFilesetSpec`\n",
    "* `FieldSpec` (intermediate field derivatives)\n",
    "* `InputFieldSpec`\n",
    "* `OutputFieldSpec`\n",
    "\n",
    "Instead of setting the data specification directly, data-spec objects are appended to the specifications of base classes (by the meta-class) by defining the `add_data_specs` class attribute. This enables the data specifications of base classes to be altered and overwritten.\n",
    "\n",
    "Each data-spec object is given a name (i.e. the one that appears on the class \"menu\") and assigned a file-format (filesets) or data type (fields). The key difference between input and output (and intermediate) data-specs is that output data-specs refer to the \"pipeline constructor method\" that constructs a pipeline to generate them. Intermediate specs are equivalent to output specs in every respect except they appear don't appear on the class menu by default.\n",
    "\n",
    "Typically, the only thing you will ever need to do with a data-spec is initialise it and append it to the `add_data_specs` list, and the best place to see the available initialisation options is the doc strings, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputFilesetSpec:\n",
      " \n",
      "    A specification for an \"acquired\" fileset (e.g from the scanner or\n",
      "    standard atlas)\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str\n",
      "        The name of the fileset\n",
      "    valid_formats : FileFormat | list[FileFormat]\n",
      "        The acceptable file formats for input filesets to match this spec\n",
      "    frequency : str\n",
      "        One of 'per_session', 'per_subject', 'per_visit' or 'per_dataset',\n",
      "        specifying whether the fileset is present for each session, subject,\n",
      "        visit or dataset.\n",
      "    desc : str\n",
      "        Description of what the field represents\n",
      "    optional : bool\n",
      "        Whether the specification is optional or not. Only valid for\n",
      "        \"acquired\" fileset specs.\n",
      "    default : FilesetSlice | object\n",
      "        The default value to be passed as an input to this spec if none are\n",
      "        provided. Can either be an explicit FilesetSlice or any object\n",
      "        with a 'slice' property that will return a default slice.\n",
      "        This object should also implement a 'bind(self, analysis)' method to\n",
      "        allow the analysis to be bound to it.\n",
      "    \n",
      "InputFieldSpec:\n",
      " \n",
      "    An abstract base class representing an acquired field\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str\n",
      "        The name of the fileset\n",
      "    dtype : type\n",
      "        The datatype of the value. Can be one of (float, int, str)\n",
      "    frequency : str\n",
      "        One of 'per_session', 'per_subject', 'per_visit' or 'per_dataset',\n",
      "        specifying whether the fileset is present for each session, subject,\n",
      "        visit or dataset.\n",
      "    desc : str\n",
      "        Description of what the field represents\n",
      "    optional : bool\n",
      "        Whether the specification is optional or not. Only valid for\n",
      "        \"acquired\" fileset specs.\n",
      "    default : FieldSlice | callable\n",
      "        The default value to be passed as an input to this spec if none are\n",
      "        provided. Can either be an explicit FieldSlice or any object\n",
      "        with a 'slice' property that will return a default slice.\n",
      "        This object should also implement a 'bind(self, analysis)' method to\n",
      "        allow the analysis to be bound to it.\n",
      "    \n",
      "FilesetSpec:\n",
      " \n",
      "    A specification for a fileset within a analysis to be derived from a\n",
      "    processing pipeline.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str\n",
      "        The name of the fileset\n",
      "    format : FileFormat\n",
      "        The file format used to store the fileset. Can be one of the\n",
      "        recognised formats\n",
      "    pipeline_getter : str\n",
      "        Name of the method in the analysis that constructs a pipeline to derive\n",
      "        the fileset\n",
      "    frequency : str\n",
      "        One of 'per_session', 'per_subject', 'per_visit' or 'per_dataset',\n",
      "        specifying whether the fileset is present for each session, subject,\n",
      "        visit or dataset.\n",
      "    desc : str\n",
      "        Description of what the field represents\n",
      "    valid_formats : list[FileFormat]\n",
      "        A list of valid file formats that can be supplied to the spec if\n",
      "        overridden as an input. Typically not required, but useful for some\n",
      "        specs that are typically provided as inputs (e.g. magnitude MRI)\n",
      "        but can be derived from other inputs (e.g. coil-wise MRI images)\n",
      "    pipeline_args : dct[str, *] | None\n",
      "        Arguments to pass to the pipeline constructor method. Avoids having to\n",
      "        create separate methods for each spec, where the only difference\n",
      "        between the specs are interface parameterisations\n",
      "    group : str | None\n",
      "        A name for a group of fileset specs. Used improve human searching of\n",
      "        available options\n",
      "    \n",
      "FieldSpec:\n",
      " \n",
      "    An abstract base class representing the specification for a derived\n",
      "    fileset.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str\n",
      "        The name of the fileset\n",
      "    dtype : type\n",
      "        The datatype of the value. Can be one of (float, int, str)\n",
      "    pipeline_getter : str\n",
      "        Name of the method that constructs pipelines to derive the field\n",
      "    frequency : str\n",
      "        One of 'per_session', 'per_subject', 'per_visit' or 'per_dataset',\n",
      "        specifying whether the fileset is present for each session, subject,\n",
      "        visit or dataset.\n",
      "    desc : str\n",
      "        Description of what the field represents\n",
      "    pipeline_args : dct[str, *] | None\n",
      "        Arguments to pass to the pipeline constructor method. Avoids having to\n",
      "        create separate methods for each spec, where the only difference\n",
      "        between the specs are interface parameterisations\n",
      "    group : str\n",
      "        A name for a group of fileset specs. Used improve human searching of\n",
      "        available options\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from arcana import InputFilesetSpec, InputFieldSpec, FilesetSpec, FieldSpec\n",
    "print('InputFilesetSpec:\\n', InputFilesetSpec.__doc__)\n",
    "print('InputFieldSpec:\\n', InputFieldSpec.__doc__)\n",
    "print('FilesetSpec:\\n', FilesetSpec.__doc__)\n",
    "print('FieldSpec:\\n', FieldSpec.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key parameters to note are:\n",
    "\n",
    "#### file_format/datatype\n",
    "\n",
    "The format/data-type that the item will be converted to when it is stored in the dataset\n",
    "\n",
    "#### frequency\n",
    "\n",
    "Where the \"data-items\" sit in the dataset tree, i.e. whether there is one for every session, subject, visit or the whole dataset. Valid values for `frequency` are\n",
    "\n",
    "* 'per_session'\n",
    "* 'per_subject'\n",
    "* 'per_visit'\n",
    "* 'per_dataset'\n",
    "\n",
    "#### pipeline_getter\n",
    "\n",
    "The name of the method in the class that constructs the pipeline to generate the derivatives\n",
    "\n",
    "### Example\n",
    "\n",
    "In the Basic-Brain Analysis class we define three output, one input and one intermediate fileset specs as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__main__.MyBasicBrainAnalysis Menu \n",
      "-----------------------------------\n",
      "\n",
      "Inputs:\n",
      "    magnitude : nifti_gz\n",
      "        A magnitude image (e.g. T1w, T2w, etc..)\n",
      "\n",
      "Intermediate:\n",
      "    brain_mask : nifti_gz\n",
      "        Brain mask used for skull-stripping\n",
      "\n",
      "Outputs:\n",
      "    brain : nifti_gz\n",
      "        Skull-stripped magnitude image\n",
      "    smooth : nifti_gz\n",
      "        Smoothed magnitude image\n",
      "    smooth_masked : nifti_gz\n",
      "        Smoothed and masked magnitude image\n",
      "\n",
      "Parameters:\n"
     ]
    }
   ],
   "source": [
    "from arcana import InputFilesetSpec, FilesetSpec, OutputFilesetSpec\n",
    "from banana.file_format import nifti_gz_format\n",
    "\n",
    "class MyBasicBrainAnalysis(Analysis, metaclass=AnalysisMetaClass):\n",
    "\n",
    "    add_data_specs = [\n",
    "        InputFilesetSpec('magnitude', nifti_gz_format,\n",
    "                         desc=\"A magnitude image (e.g. T1w, T2w, etc..)\"),\n",
    "        OutputFilesetSpec('brain', nifti_gz_format,\n",
    "                          'brain_extraction_pipeline',\n",
    "                          desc=\"Skull-stripped magnitude image\"),\n",
    "        FilesetSpec('brain_mask', nifti_gz_format,\n",
    "                    'brain_extraction_pipeline',\n",
    "                    desc=\"Brain mask used for skull-stripping\"),\n",
    "        OutputFilesetSpec('smooth', nifti_gz_format, 'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed magnitude image\"),\n",
    "        OutputFilesetSpec('smooth_masked', nifti_gz_format,\n",
    "                          'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed and masked magnitude image\")]\n",
    "    \n",
    "    def brain_extraction_pipeline(self, **name_maps):\n",
    "        \"We'll define this later\"\n",
    "    \n",
    "    def smooth_mask_pipeline(self, **name_maps):\n",
    "        \"We'll define this later\"\n",
    "\n",
    "print(MyBasicBrainAnalysis.static_menu(full=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** how the 'pipeline_getter' parameters of the spec objects reference the name of a \"pipeline constructor\" method defined in the class. The matches between these names are checked by the metaclass so if we don't define them Arcana will throw an error. \n",
    "\n",
    "Since `MyBasicBrainAnalysis` inherits directly from `Analysis` the only specs in the data specification are those in `add_data_specs`. However, if we would like to extend `MyBasicBrainAnalysis` to make a new class `MyExtendedBasicBrainAnalysis` we can add to and override the specs from `MyBasicBrainAnalysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__main__.MyExtendedBasicBrainAnalysis Menu \n",
      "-------------------------------------------\n",
      "\n",
      "Inputs:\n",
      "    magnitude : nifti_gz\n",
      "        A magnitude image (e.g. T1w, T2w, etc..)\n",
      "\n",
      "Intermediate:\n",
      "    brain_mask : nifti_gz\n",
      "        Brain mask used for skull-stripping\n",
      "\n",
      "Outputs:\n",
      "    brain : nifti_gz\n",
      "        Skull-stripped magnitude image\n",
      "    smooth : mrtrix_image\n",
      "        Smoothed magnitude image in Mrtrix format\n",
      "    smooth_masked : nifti_gz\n",
      "        Smoothed and masked magnitude image\n",
      "    thresholded : nifti_gz\n",
      "        Thresholded smoothed magnitude image\n",
      "\n",
      "Parameters:\n"
     ]
    }
   ],
   "source": [
    "from banana.file_format import mrtrix_image_format\n",
    "\n",
    "class MyExtendedBasicBrainAnalysis(MyBasicBrainAnalysis, metaclass=AnalysisMetaClass):\n",
    "\n",
    "    add_data_specs = [\n",
    "        OutputFilesetSpec('smooth', mrtrix_image_format, 'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed magnitude image in Mrtrix format\"),\n",
    "        OutputFilesetSpec('thresholded', nifti_gz_format,\n",
    "                          'threshold_pipeline',\n",
    "                          desc=\"Thresholded smoothed magnitude image\")]\n",
    "    \n",
    "    def threshold_pipeline(self, **name_maps):\n",
    "        \"We'll define this later\"\n",
    "        \n",
    "print(MyExtendedBasicBrainAnalysis.static_menu(full=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is now a `thresholded` output, and the `smooth` image is now of `mrtrix_image` format instead of `nifti_gz`. \n",
    "\n",
    "**Note**: we can get away with changing the format of the `smooth` from zipped NiFTI to MRtrix because if the output format of the pipeline doesn't match the format of the specification it will be automatically converted before it is stored in the dataset (as long as a converter exists)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Specification\n",
    "\n",
    "The parameter specification works very much like the data specification but for parameters. \"Param-specs\" can be either of class `ParamSpec` or `SwitchSpec` type.\n",
    "\n",
    "`SwitchSpec`s are used to qualitatively change the analysis performed, e.g. using FSL for non-linear registration to a template (i.e. FNIRT) instead of ANTs. `ParamSpec`s are used to quantitatively change the analysis, e.g. change the required threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParamSpec:\n",
      " \n",
      "    Specifies a parameter that can be passed to the analysis\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str\n",
      "        Name of the parameter\n",
      "    default : float | int | str | list | tuple\n",
      "        Default value of the parameter\n",
      "    choices : List(float | int | str | list | tuple) | None\n",
      "        Restrict valid inputs to the following choices\n",
      "    desc : str\n",
      "        A description of the parameter\n",
      "    dtype : type | None\n",
      "        The datatype of the parameter. If none will be determined from\n",
      "        default value\n",
      "    \n",
      "SwitchSpec:\n",
      " \n",
      "    Specifies a special parameter that switches between different\n",
      "    methods and/or pipeline input/outputs. Typically used to select\n",
      "    between comparable methods (e.g. FSL or ANTs registration) but can\n",
      "    also be used to specify whether certain methods are applied, and by\n",
      "    extension some auxiliary outputs are generated\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str\n",
      "        Name of the parameter\n",
      "    default : str\n",
      "        Default option for the switch\n",
      "    choices : list[str]\n",
      "        The valid values for the switch\n",
      "    desc : str\n",
      "        A description of the parameter\n",
      "    fallbacks : dict[str, str]\n",
      "        A map between a switch choice and the value it can be assumed to be\n",
      "        if it isn't handled explicitly. Note that this relies on tests for the\n",
      "        more specific choice to peformed first.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from arcana import ParamSpec, SwitchSpec\n",
    "\n",
    "print('ParamSpec:\\n', ParamSpec.__doc__)\n",
    "print('SwitchSpec:\\n', SwitchSpec.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the data specification, instead of setting the parameter specification directly it is added to the class via `add_param_specs` to allow manipulation by subclasses.\n",
    "\n",
    "Returning to the `BasicBrainAnalysis` example we add in the FWHM parameter used in the smoothing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__main__.BasicBrainAnalysis Menu \n",
      "---------------------------------\n",
      "\n",
      "Inputs:\n",
      "    magnitude : nifti_gz\n",
      "        A magnitude image (e.g. T1w, T2w, etc..)\n",
      "\n",
      "Outputs:\n",
      "    brain : nifti_gz\n",
      "        Skull-stripped magnitude image\n",
      "    smooth : nifti_gz\n",
      "        Smoothed magnitude image\n",
      "    smooth_masked : nifti_gz\n",
      "        Smoothed and masked magnitude image\n",
      "\n",
      "Parameters:\n",
      "    smoothing_fwhm : float (4.0)\n",
      "        The full-width-half-maxium radius of the smoothing kernel\n"
     ]
    }
   ],
   "source": [
    "class BasicBrainAnalysis(Analysis, metaclass=AnalysisMetaClass):\n",
    "    \"\"\"\n",
    "    A baisc example that demonstrates how Analysis classes work.\n",
    "    \"\"\"\n",
    "\n",
    "    add_data_specs = [\n",
    "        InputFilesetSpec('magnitude', nifti_gz_format,\n",
    "                         desc=\"A magnitude image (e.g. T1w, T2w, etc..)\"),\n",
    "        OutputFilesetSpec('brain', nifti_gz_format,\n",
    "                          'brain_extraction_pipeline',\n",
    "                          desc=\"Skull-stripped magnitude image\"),\n",
    "        FilesetSpec('brain_mask', nifti_gz_format,\n",
    "                    'brain_extraction_pipeline',\n",
    "                    desc=\"Brain mask used for skull-stripping\"),\n",
    "        OutputFilesetSpec('smooth', nifti_gz_format, 'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed magnitude image\"),\n",
    "        OutputFilesetSpec('smooth_masked', nifti_gz_format,\n",
    "                          'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed and masked magnitude image\")]\n",
    "\n",
    "    add_param_specs = [\n",
    "        ParamSpec('smoothing_fwhm', 4.0,\n",
    "                  desc=(\"The full-width-half-maxium radius of the smoothing \"\n",
    "                        \"kernel\"))]\n",
    "\n",
    "    def brain_extraction_pipeline(self, **name_maps):\n",
    "        \"We'll define this later\"\n",
    "    \n",
    "    def smooth_mask_pipeline(self, **name_maps):\n",
    "        \"We'll define this later\"\n",
    "\n",
    "print(BasicBrainAnalysis.static_menu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Constructor Methods\n",
    "\n",
    "Pipeline constructor methods are where the action happens in Analysis classes. They return `Pipeline` objects (which are just thin wrappers around `nipype.Workflows`) that link the data specification together by taking one or more data-specs as inputs and generating one or more as outputs.\n",
    "\n",
    "### Initialising a pipeline\n",
    "\n",
    "The basic form of a pipeline constructor method is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from banana.citation import fsl_cite\n",
    "\n",
    "def smooth_mask_pipeline(self, **name_maps):\n",
    "\n",
    "    pipeline = self.new_pipeline(\n",
    "        'smooth_mask',\n",
    "        desc=\"Smooths and masks a brain image\",\n",
    "        name_maps=name_maps,\n",
    "        citations=[fsl_cite])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the `new_pipeline` creates the `Pipeline` object, which is returned at the end of the method. Looking at the doc string of the `new_pipeline` we can see the parameters that you need/can pass to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Creates a Pipeline object, passing the analysis (self) as the first\n",
      "        argument\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        name : str\n",
      "            The name of the pipeline\n",
      "        desc : str\n",
      "            A description of what the pipeline does\n",
      "        ciations : List[Citation]\n",
      "            List of scientific papers that describe the workflow and should be\n",
      "            cited in publications that use it\n",
      "        name_maps : dict\n",
      "            A dictionary containing several modifying keyword arguments that\n",
      "            manipulate way the pipeline is constructed (e.g. map inputs and\n",
      "            outputs to new entries in the data specification table). Typically\n",
      "            names of inputs, outputs and the pipeline itself. Intended to allow\n",
      "            secondary pipeline constructors to call a constructor, and return a\n",
      "            modified version of the pipeline it returns.\n",
      "\n",
      "            It should be passed directly from wildcard keyword args passed to\n",
      "            the pipeline constructor, e.g.\n",
      "\n",
      "            def my_pipeline_constructor(self, **name_maps):\n",
      "                pipeline = self.new_pipeline('my_pipeline', name_maps=name_maps)\n",
      "                pipeline.add('a_node', MyInterface())\n",
      "\n",
      "                ...\n",
      "\n",
      "                return pipeline\n",
      "\n",
      "            The keywords in 'name_maps' used in pipeline construction are:\n",
      "\n",
      "            name : str\n",
      "                A new name for the pipeline\n",
      "            prefix : str\n",
      "                Prefix prepended to the original name of the pipeline.\n",
      "                Typically only one of name and prefix is used at each nested\n",
      "                level, but they can be used in conjunction.\n",
      "            input_map : str | dict[str,str]\n",
      "                Applied to the input names used by the pipeline to map them to\n",
      "                new entries of the data specification in modified pipeline\n",
      "                constructors. Typically used in sub-class or multi-analysis. If\n",
      "                a string, the map is interpreted as a prefix applied to the\n",
      "                names given in the original pipeline, if it is a dictionary the\n",
      "                names are mapped explicitly.\n",
      "            output_map : str | dict[str,str]\n",
      "                Same as the input map but applied to outputs instead of inputs\n",
      "                to the pipeline.\n",
      "            name_maps : dict\n",
      "                Modifications from nested pipeline constructors\n",
      "            analysis : Analysis\n",
      "                A different analysis to bind the pipeline to from the one\n",
      "                containing the inner pipeline constructor. Intended to be used\n",
      "                with multi-studies.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(Analysis.new_pipeline.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `name` parameter is just used internally to distinguish working directories between pipeline nodes. There aren't any requirements on it except that needs to be unique amongst all pipelines that can be generated by the Analysis instance.\n",
    "\n",
    "`citations` lists the publications that should be cited when using this pipeline. However, I plan to replace this bespoke solution with the third-party package [duecredit](https://pypi.org/project/duecredit/), which nipype uses.\n",
    "\n",
    "The `name_maps` parameter should be passed directly from the keyword arguments of the pipeline constructor. It allows sub-classes to do funky things when manipulating methods defined in base classes such as mapping inputs and outputs onto different data-specs. However, going into the details of how it works is beyond the scope of this notebook.\n",
    "\n",
    "### Adding nodes to a pipeline\n",
    "\n",
    "The syntax for adding nodes to a pipeline is somewhat different to how it is done in Nipype. This is because it is modelled on the proposed syntax for [Nipype v2.0](https://github.com/nipy/nipype/projects/8), which aims to streamline code for workflow construction and make it easy to read.\n",
    "\n",
    "Nodes are added to a pipeline using the `add` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Adds a processing Node to the pipeline\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        name : str\n",
      "            Name for the node\n",
      "        interface : nipype.Interface\n",
      "            The interface to use for the node\n",
      "        inputs : dict[str, (str, FileFormat) | (Node, str)]\n",
      "            Connections from inputs of the pipeline and outputs of other nodes\n",
      "            to inputs of node. The keys of the dictionary are the field names\n",
      "            and the values are 2-tuple containing either the name of the data\n",
      "            spec and the data format it is expected in for pipeline inputs or\n",
      "            the sending Node and the the name of an output of the sending Node.\n",
      "            Note that pipeline inputs can be specified outside this method\n",
      "            using the 'connect_input' method and connections between nodes with\n",
      "            the the 'connect' method.\n",
      "        outputs : dict[str, (str, FileFormat)]\n",
      "            Connections to outputs of the pipeline from fields of the\n",
      "            interface. The keys of the dictionary are the names of the data\n",
      "            specs that will be written to and the values are the interface\n",
      "            field name and the data format it is produced in. Note that output\n",
      "            connections can also be specified using the 'connect_output'\n",
      "            method.\n",
      "        requirements : list(Requirement)\n",
      "            List of required packages need for the node to run (default: [])\n",
      "        wall_time : float\n",
      "            Time required to execute the node in minutes (default: 1)\n",
      "        mem_gb : int\n",
      "            Required memory for the node in GB\n",
      "        n_procs : int\n",
      "            Preferred number of threads to run the node on (default: 1)\n",
      "        annotations : dict[str, *]\n",
      "            Additional annotations to add to the node, which may be used by\n",
      "            the Processor node to optimise execution (e.g. 'gpu': True)\n",
      "        iterfield : str\n",
      "            Name of field to be passed an iterable to iterator over.\n",
      "            If present, a MapNode will be created instead of a regular node\n",
      "        joinsource : str\n",
      "            Name of iterator field to join. Typically one of the implicit\n",
      "            iterators (i.e. Analysis.SUBJECT_ID or Analysis.VISIT_ID)\n",
      "            to join over the subjects and/or visits\n",
      "        joinfield : str\n",
      "            Name of field to pass the joined list when creating a JoinNode\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        node : Node\n",
      "            The Node object that has been added to the pipeline\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from arcana.pipeline import Pipeline\n",
    "print(Pipeline.add.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which in our BasicBrainAnalysis example looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_mask_pipeline(self, **name_maps):\n",
    "\n",
    "    pipeline = self.new_pipeline(\n",
    "        'smooth_mask',\n",
    "        desc=\"Smooths and masks a brain image\",\n",
    "        name_maps=name_maps,\n",
    "        citations=[fsl_cite])\n",
    "\n",
    "    # Smoothing process\n",
    "    smooth = pipeline.add(\n",
    "        'smooth',\n",
    "        fsl.IsotropicSmooth(\n",
    "            fwhm=self.parameter('smoothing_fwhm')),           # Param. passed from param-spec\n",
    "        inputs={\n",
    "            'in_file': ('magnitude', nifti_gz_format)},       # Input from data-spec\n",
    "        outputs={\n",
    "            'smooth': ('out_file', nifti_gz_format)},         # Output to data-spec\n",
    "        requirements=[\n",
    "            fsl_req.v('5.0.10')])                             # Requires FSL >= 5.0.10\n",
    "    \n",
    "    pipeline.add(\n",
    "        'mask',\n",
    "        fsl.ApplyMask(\n",
    "            output_datatype=int),                             # Fixed param of pipeline\n",
    "        inputs={\n",
    "            'in_file': (smooth, 'out_file'),                  # Input from previous node\n",
    "            'mask_file': ('brain_mask', nifti_gz_format)},    # Input from data-spec\n",
    "        outputs={\n",
    "            'smooth_masked': ('out_file', nifti_gz_format)},  # Output to data-spec\n",
    "        requirements=[\n",
    "            fsl_req.v('5.0.10')])                             # Requires FSL >= 5.0.10\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key points to note are:\n",
    "\n",
    "* All nodes need a unique name within the pipeline (as in Nipype)\n",
    "* Stylistic convention dictates that constant interface traits are set when the interface is initialised\n",
    "* `inputs` is a dictionary that maps the name of an input-trait of the interface to a 2-tuple consisting of either\n",
    "  * a data-spec name (i.e. inputs of the pipeline) and the file-format/datatype the input is expected in (the format will be automatically converted if required)\n",
    "  * an upstream node and name of the trait to connect from the upstream node\n",
    "* `outputs` is a dictionary that maps data-spec names (i.e. outputs of the pipeline) to a 2-tuple consisting of the name of an output-trait and the file-format/datatype it is produced in.\n",
    "* `requirements` are a list of `arcana.Requirement` objects that specify versions of external packages (e.g. FSL, SPM, MRtrix) that are required for the node to run.\n",
    "\n",
    "### Merging and Splitting Pipelines Across Subjects/Visits\n",
    "\n",
    "Arcana handles iteration over subjects and sessions in the background as implicitly specifed by the frequencies of inputs and outputs of the pipeline. However, in some cases you may need to join over all subjects/visits to create a summary statistic (e.g. mean), and then potentially use this variable back on an individual subject/visit level again (e.g. normalisation). As we saw in the in the morning \"Advanced Nipype\" section, these cases are handled by Nipype using iterators, map nodes and join nodes.\n",
    "\n",
    "In Arcana join nodes are specified by providing the `joinsource` an `joinfield` parameters of when adding a node to a pipeline. These parameters work the same as they do for Nipype map nodes. Access to Arcana's implicit iterator nodes are exposed via the `self.SUBJECT_ID` and `self.VISIT_ID` variables. For example, in the `statistics_pipeline` of the `example.analysis.ToyAnalysis` we do a two-step merge, first over visits and then subjects to create the *per_dataset* metrics 'average' and 'std_dev' from the *per_session* 'selected_metric'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_pipeline(self, **name_maps):\n",
    "    pipeline = self.new_pipeline(\n",
    "        name='statistics',\n",
    "        name_maps=name_maps,\n",
    "        desc=\"Calculate statistics\")\n",
    "\n",
    "    merge_visits = pipeline.add(\n",
    "        'merge_visits',\n",
    "        Merge(\n",
    "            numinputs=1),\n",
    "        inputs={\n",
    "            'in1': ('selected_metric', text_format)},\n",
    "        joinsource=self.VISIT_ID,\n",
    "        joinfield=['in1'])\n",
    "\n",
    "    merge_subjects = pipeline.add(\n",
    "        'merge_subjects',\n",
    "        Merge(\n",
    "            numinputs=1,\n",
    "            ravel_inputs=True),\n",
    "        inputs={\n",
    "            'in1': (merge_visits, 'out')},\n",
    "        joinsource=self.SUBJECT_ID,\n",
    "        joinfield=['in1'])\n",
    "\n",
    "    concat = pipeline.add(\n",
    "        'concat',\n",
    "        ConcatFloats(),\n",
    "        inputs={\n",
    "            'in_files': (merge_subjects, 'out')})\n",
    "\n",
    "    pipeline.add(\n",
    "        'extract_metrics',\n",
    "        ExtractMetrics(),\n",
    "        inputs={\n",
    "            'in_list': (concat, 'out_list')},\n",
    "        outputs={\n",
    "            'average': ('avg', float),\n",
    "            'std_dev': ('std', float)})\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`self.SUBJECT_ID` and `self.VISIT_ID` can also be used to expand over all subject/visits again and to create a map node, simply provide the `iterfield` parameter when adding a node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "We can now put the three key components together (i.e. data & parameter specifications and pipeline constructor methods) to make a fully-functioning Analysis class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBrainAnalysis(Analysis, metaclass=AnalysisMetaClass):\n",
    "    \"\"\"\n",
    "    A baisc analysis class that demonstrates how Analysis classes work.\n",
    "    \"\"\"\n",
    "\n",
    "    add_data_specs = [\n",
    "        InputFilesetSpec('magnitude', nifti_gz_format,\n",
    "                         desc=\"A magnitude image (e.g. T1w, T2w, etc..)\"),\n",
    "        OutputFilesetSpec('brain', nifti_gz_format,\n",
    "                          'brain_extraction_pipeline',\n",
    "                          desc=\"Skull-stripped magnitude image\"),\n",
    "        FilesetSpec('brain_mask', nifti_gz_format,\n",
    "                    'brain_extraction_pipeline',\n",
    "                    desc=\"Brain mask used for skull-stripping\"),\n",
    "        OutputFilesetSpec('smooth', nifti_gz_format, 'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed magnitude image\"),\n",
    "        OutputFilesetSpec('smooth_masked', nifti_gz_format,\n",
    "                          'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed and masked magnitude image\")]\n",
    "\n",
    "    add_param_specs = [\n",
    "        ParamSpec('smoothing_fwhm', 4.0,\n",
    "                  desc=(\"The full-width-half-maxium radius of the smoothing \"\n",
    "                        \"kernel\"))]\n",
    "\n",
    "    def brain_extraction_pipeline(self, **name_maps):\n",
    "\n",
    "        pipeline = self.new_pipeline(\n",
    "            'brain_extraction',\n",
    "            desc=\"Extracts brain from full-head image\",\n",
    "            name_maps=name_maps,\n",
    "            citations=[fsl_cite])\n",
    "\n",
    "        pipeline.add(\n",
    "            'bet',\n",
    "            fsl.BET(\n",
    "                mask=True),\n",
    "            inputs={\n",
    "                'in_file': ('magnitude', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'brain': ('out_file', nifti_gz_format),\n",
    "                'brain_mask': ('mask_file', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        return pipeline\n",
    "\n",
    "    def smooth_mask_pipeline(self, **name_maps):\n",
    "\n",
    "        pipeline = self.new_pipeline(\n",
    "            'smooth_mask',\n",
    "            desc=\"Smooths and masks a brain image\",\n",
    "            name_maps=name_maps,\n",
    "            citations=[fsl_cite])\n",
    "\n",
    "        # Smoothing process\n",
    "        smooth = pipeline.add(\n",
    "            'smooth',\n",
    "            fsl.IsotropicSmooth(\n",
    "                fwhm=self.parameter('smoothing_fwhm')),\n",
    "            inputs={\n",
    "                'in_file': ('magnitude', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'smooth': ('out_file', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        pipeline.add(\n",
    "            'mask',\n",
    "            fsl.ApplyMask(),\n",
    "            inputs={\n",
    "                'in_file': (smooth, 'out_file'),\n",
    "                'mask_file': ('brain_mask', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'smooth_masked': ('out_file', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to Create Publication Outputs\n",
    "\n",
    "While not necessary, if you are creating a new Analysis class for your specific study, it is a nice idea to implement additional methods to generate all your publication outputs (figures, stats, etc...) within the Analysis class.\n",
    "\n",
    "For example in the `BasicBrainAnalysis` class we have the method `plot_comparison`, which is implemented as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BasicBrainAnalysis(Analysis, metaclass=AnalysisMetaClass):\n",
    "    \"\"\"\n",
    "    A baisc analysis class that demonstrates how Analysis classes work.\n",
    "    \"\"\"\n",
    "\n",
    "    add_data_specs = [\n",
    "        InputFilesetSpec('magnitude', nifti_gz_format,\n",
    "                         desc=\"A magnitude image (e.g. T1w, T2w, etc..)\"),\n",
    "        OutputFilesetSpec('brain', nifti_gz_format,\n",
    "                          'brain_extraction_pipeline',\n",
    "                          desc=\"Skull-stripped magnitude image\"),\n",
    "        FilesetSpec('brain_mask', nifti_gz_format,\n",
    "                    'brain_extraction_pipeline',\n",
    "                    desc=\"Brain mask used for skull-stripping\"),\n",
    "        OutputFilesetSpec('smooth', nifti_gz_format, 'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed magnitude image\"),\n",
    "        OutputFilesetSpec('smooth_masked', nifti_gz_format,\n",
    "                          'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed and masked magnitude image\")]\n",
    "\n",
    "    add_param_specs = [\n",
    "        ParamSpec('smoothing_fwhm', 4.0,\n",
    "                  desc=(\"The full-width-half-maxium radius of the smoothing \"\n",
    "                        \"kernel\"))]\n",
    "\n",
    "    def brain_extraction_pipeline(self, **name_maps):\n",
    "\n",
    "        pipeline = self.new_pipeline(\n",
    "            'brain_extraction',\n",
    "            desc=\"Extracts brain from full-head image\",\n",
    "            name_maps=name_maps,\n",
    "            citations=[fsl_cite])\n",
    "\n",
    "        pipeline.add(\n",
    "            'bet',\n",
    "            fsl.BET(\n",
    "                mask=True),\n",
    "            inputs={\n",
    "                'in_file': ('magnitude', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'brain': ('out_file', nifti_gz_format),\n",
    "                'brain_mask': ('mask_file', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        return pipeline\n",
    "\n",
    "    def smooth_mask_pipeline(self, **name_maps):\n",
    "\n",
    "        pipeline = self.new_pipeline(\n",
    "            'smooth_mask',\n",
    "            desc=\"Smooths and masks a brain image\",\n",
    "            name_maps=name_maps,\n",
    "            citations=[fsl_cite])\n",
    "\n",
    "        # Smoothing process\n",
    "        smooth = pipeline.add(\n",
    "            'smooth',\n",
    "            fsl.IsotropicSmooth(\n",
    "                fwhm=self.parameter('smoothing_fwhm')),\n",
    "            inputs={\n",
    "                'in_file': ('magnitude', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'smooth': ('out_file', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        pipeline.add(\n",
    "            'mask',\n",
    "            fsl.ApplyMask(),\n",
    "            inputs={\n",
    "                'in_file': (smooth, 'out_file'),\n",
    "                'mask_file': ('brain_mask', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'smooth_masked': ('out_file', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        return pipeline\n",
    "\n",
    "    def plot_comparision(self, figsize=(12, 4)):\n",
    "\n",
    "        for subj_i in self.subject_ids:\n",
    "            for visit_i in self.visit_ids:\n",
    "                f = plt.figure(figsize=figsize)\n",
    "                f.suptitle('Subject \"{}\" - Visit \"{}\"'.format(subj_i, visit_i))\n",
    "                for i, spec_name in enumerate(['magnitude', 'smooth',\n",
    "                                               'brain_mask', 'smooth_masked']):\n",
    "                    f.add_subplot(1, 4, i + 1)\n",
    "                    self._plot_slice(spec_name, subj_i, visit_i)\n",
    "                    plt.title(spec_name)\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_slice(self, spec_name, subject_id=None, visit_id=None):\n",
    "        # Load the image\n",
    "        data = self.data(spec_name, derive=True).item(\n",
    "            subject_id=subject_id, visit_id=visit_id).get_array()\n",
    "\n",
    "        # Cut in the middle of the brain\n",
    "        cut = int(data.shape[-1] / 2) + 10\n",
    "\n",
    "        # Plot the data\n",
    "        plt.imshow(np.rot90(data[..., cut]), cmap=\"gray\")\n",
    "        plt.gca().set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the `_plot_slice` method access the derived data using the `Analysis.data` method like we did in the \"Applying Analysis Class\" notebook. From the `FilesetSlice` it returns you can access a single data \"item\" using the `item` method. In Banana, the data array of `Fileset`s in standard image format can be accessed using the `get_arrray` method, which we then plot with Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Extend the `example.analaysis.BasicBrainAnalysis` class to add the `image_std` data-spec using the `nipype.interfaces.fsl.ImageStats` interface, which is the standard deviation of the smooth-masked image. Then run this analysis on the Tw-weighted images in the 'output/sample-datasets/depth1' dataset created in the \"Applying Analysis Classes\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: fslstats [preoptions] <input> [options]\r\n",
      "\r\n",
      "preoption -t will give a separate output line for each 3D volume of a 4D timeseries\r\n",
      "preoption -K < indexMask > will generate seperate n submasks from indexMask, for indexvalues 1..n where n is the maximum index value in indexMask, and generate statistics for each submask\r\n",
      "Note - options are applied in order, e.g. -M -l 10 -M will report the non-zero mean, apply a threshold and then report the new nonzero mean\r\n",
      "\r\n",
      "-l <lthresh> : set lower threshold\r\n",
      "-u <uthresh> : set upper threshold\r\n",
      "-r           : output <robust min intensity> <robust max intensity>\r\n",
      "-R           : output <min intensity> <max intensity>\r\n",
      "-e           : output mean entropy ; mean(-i*ln(i))\r\n",
      "-E           : output mean entropy (of nonzero voxels)\r\n",
      "-v           : output <voxels> <volume>\r\n",
      "-V           : output <voxels> <volume> (for nonzero voxels)\r\n",
      "-m           : output mean\r\n",
      "-M           : output mean (for nonzero voxels)\r\n",
      "-s           : output standard deviation\r\n",
      "-S           : output standard deviation (for nonzero voxels)\r\n",
      "-w           : output smallest ROI <xmin> <xsize> <ymin> <ysize> <zmin> <zsize> <tmin> <tsize> containing nonzero voxels\r\n",
      "-x           : output co-ordinates of maximum voxel\r\n",
      "-X           : output co-ordinates of minimum voxel\r\n",
      "-c           : output centre-of-gravity (cog) in mm coordinates\r\n",
      "-C           : output centre-of-gravity (cog) in voxel coordinates\r\n",
      "-p <n>       : output nth percentile (n between 0 and 100)\r\n",
      "-P <n>       : output nth percentile (for nonzero voxels)\r\n",
      "-a           : use absolute values of all image intensities\r\n",
      "-n           : treat NaN or Inf as zero for subsequent stats\r\n",
      "-k <mask>    : use the specified image (filename) for masking - overrides lower and upper thresholds\r\n",
      "-d <image>   : take the difference between the base image and the image specified here\r\n",
      "-h <nbins>   : output a histogram (for the thresholded/masked voxels only) with nbins\r\n",
      "-H <nbins> <min> <max>   : output a histogram (for the thresholded/masked voxels only) with nbins and histogram limits of min and max\r\n",
      "\r\n",
      "Note - thresholds are not inclusive ie lthresh<allowed<uthresh\r\n"
     ]
    }
   ],
   "source": [
    "! fslstats -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__main__.MyExtendedBasicBrainAnalysis Menu \n",
      "-------------------------------------------\n",
      "\n",
      "Inputs:\n",
      "    magnitude : nifti_gz\n",
      "        A magnitude image (e.g. T1w, T2w, etc..)\n",
      "\n",
      "Intermediate:\n",
      "    brain_mask : nifti_gz\n",
      "        Brain mask used for skull-stripping\n",
      "\n",
      "Outputs:\n",
      "    brain : nifti_gz\n",
      "        Skull-stripped magnitude image\n",
      "    image_std : float\n",
      "        Standard deviation of the smoothed masked image\n",
      "    smooth : mrtrix_image\n",
      "        Smoothed magnitude image in Mrtrix format\n",
      "    smooth_masked : nifti_gz\n",
      "        Smoothed and masked magnitude image\n",
      "\n",
      "Parameters:\n",
      "    smoothing_fwhm : float (4.0)\n",
      "        The full-width-half-maxium radius of the smoothing kernel\n"
     ]
    }
   ],
   "source": [
    "from arcana import OutputFieldSpec\n",
    "from example.analysis import BasicBrainAnalysis\n",
    "from banana.requirement import fsl_req\n",
    "from banana.citation import fsl_cite\n",
    "\n",
    "\n",
    "class MyExtendedBasicBrainAnalysis(BasicBrainAnalysis, metaclass=AnalysisMetaClass):\n",
    "\n",
    "    add_data_specs = [\n",
    "        OutputFilesetSpec('smooth', mrtrix_image_format, 'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed magnitude image in Mrtrix format\"),\n",
    "        OutputFieldSpec('image_std', float,\n",
    "                        'image_std_pipeline',\n",
    "                        desc=\"Standard deviation of the smoothed masked image\")]\n",
    "    \n",
    "    def image_std_pipeline(self, **name_maps):\n",
    "        \n",
    "        pipeline = self.new_pipeline(\n",
    "            'image_std_pipeline',\n",
    "            desc=\"Calculates the standard deviation of the smooth masked image\",\n",
    "            name_maps=name_maps,\n",
    "            citations=[fsl_cite])\n",
    "\n",
    "        pipeline.add(\n",
    "            'mask',\n",
    "            fsl.ImageStats(\n",
    "                op_string='-s'),\n",
    "            inputs={\n",
    "                'in_file': ('smooth_masked', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'image_std': ('out_stat', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        return pipeline\n",
    "        \n",
    "print(MyExtendedBasicBrainAnalysis.static_menu(full=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191115-08:22:14,308 nipype.workflow INFO:\n",
      "\t Workflow image_std_pipeline settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "191115-08:22:14,466 nipype.workflow INFO:\n",
      "\t Running serially.\n",
      "191115-08:22:14,468 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.brain_extraction.brain_extraction_per_session_source\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/_subject_id_sub1/_visit_id_VISIT/brain_extraction_per_session_source\".\n",
      "191115-08:22:14,476 nipype.workflow INFO:\n",
      "\t [Node] Running \"brain_extraction_per_session_source\" (\"arcana.repository.interfaces.RepositorySource\")\n",
      "191115-08:22:14,509 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.brain_extraction.brain_extraction_per_session_source\".\n",
      "191115-08:22:14,510 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.brain_extraction.brain_extraction_bet\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/_subject_id_sub1/_visit_id_VISIT/brain_extraction_bet\".\n",
      "191115-08:22:14,518 nipype.workflow INFO:\n",
      "\t [Node] Running \"brain_extraction_bet\" (\"nipype.interfaces.fsl.preprocess.BET\"), a CommandLine Interface with command:\n",
      "bet /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth1/sub1/sub-01_ses-test_T1w.nii.gz /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/_subject_id_sub1/_visit_id_VISIT/brain_extraction_bet/sub-01_ses-test_T1w_brain.nii.gz -m\n",
      "191115-08:22:18,438 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.brain_extraction.brain_extraction_bet\".\n",
      "191115-08:22:18,440 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.brain_extraction.brain_extraction_per_session_sink\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/_subject_id_sub1/_visit_id_VISIT/brain_extraction_per_session_sink\".\n",
      "191115-08:22:18,452 nipype.workflow INFO:\n",
      "\t [Node] Running \"brain_extraction_per_session_sink\" (\"arcana.repository.interfaces.RepositorySink\")\n",
      "191115-08:22:18,471 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.brain_extraction.brain_extraction_per_session_sink\".\n",
      "191115-08:22:18,472 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.brain_extraction.brain_extraction_per_session_source\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/_subject_id_sub2/_visit_id_VISIT/brain_extraction_per_session_source\".\n",
      "191115-08:22:18,482 nipype.workflow INFO:\n",
      "\t [Node] Running \"brain_extraction_per_session_source\" (\"arcana.repository.interfaces.RepositorySource\")\n",
      "191115-08:22:18,518 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.brain_extraction.brain_extraction_per_session_source\".\n",
      "191115-08:22:18,519 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.brain_extraction.brain_extraction_bet\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/_subject_id_sub2/_visit_id_VISIT/brain_extraction_bet\".\n",
      "191115-08:22:18,524 nipype.workflow INFO:\n",
      "\t [Node] Running \"brain_extraction_bet\" (\"nipype.interfaces.fsl.preprocess.BET\"), a CommandLine Interface with command:\n",
      "bet /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth1/sub2/sub-02_ses-test_T1w.nii.gz /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/_subject_id_sub2/_visit_id_VISIT/brain_extraction_bet/sub-02_ses-test_T1w_brain.nii.gz -m\n",
      "191115-08:22:21,198 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.brain_extraction.brain_extraction_bet\".\n",
      "191115-08:22:21,199 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.brain_extraction.brain_extraction_per_session_sink\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/_subject_id_sub2/_visit_id_VISIT/brain_extraction_per_session_sink\".\n",
      "191115-08:22:21,213 nipype.workflow INFO:\n",
      "\t [Node] Running \"brain_extraction_per_session_sink\" (\"arcana.repository.interfaces.RepositorySink\")\n",
      "191115-08:22:21,232 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.brain_extraction.brain_extraction_per_session_sink\".\n",
      "191115-08:22:21,234 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.brain_extraction.brain_extraction_per_session_source\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/_subject_id_sub3/_visit_id_VISIT/brain_extraction_per_session_source\".\n",
      "191115-08:22:21,244 nipype.workflow INFO:\n",
      "\t [Node] Running \"brain_extraction_per_session_source\" (\"arcana.repository.interfaces.RepositorySource\")\n",
      "191115-08:22:21,278 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.brain_extraction.brain_extraction_per_session_source\".\n",
      "191115-08:22:21,280 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.brain_extraction.brain_extraction_bet\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/_subject_id_sub3/_visit_id_VISIT/brain_extraction_bet\".\n",
      "191115-08:22:21,286 nipype.workflow INFO:\n",
      "\t [Node] Running \"brain_extraction_bet\" (\"nipype.interfaces.fsl.preprocess.BET\"), a CommandLine Interface with command:\n",
      "bet /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth1/sub3/sub-03_ses-test_T1w.nii.gz /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/_subject_id_sub3/_visit_id_VISIT/brain_extraction_bet/sub-03_ses-test_T1w_brain.nii.gz -m\n",
      "191115-08:22:25,518 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.brain_extraction.brain_extraction_bet\".\n",
      "191115-08:22:25,519 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.brain_extraction.brain_extraction_per_session_sink\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/_subject_id_sub3/_visit_id_VISIT/brain_extraction_per_session_sink\".\n",
      "191115-08:22:25,531 nipype.workflow INFO:\n",
      "\t [Node] Running \"brain_extraction_per_session_sink\" (\"arcana.repository.interfaces.RepositorySink\")\n",
      "191115-08:22:25,551 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.brain_extraction.brain_extraction_per_session_sink\".\n",
      "191115-08:22:25,553 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.brain_extraction.brain_extraction_per_session_subject_id_deiter\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/_visit_id_VISIT/brain_extraction_per_session_subject_id_deiter\".\n",
      "191115-08:22:25,560 nipype.workflow INFO:\n",
      "\t [Node] Running \"brain_extraction_per_session_subject_id_deiter\" (\"nipype.interfaces.utility.base.IdentityInterface\")\n",
      "191115-08:22:25,569 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.brain_extraction.brain_extraction_per_session_subject_id_deiter\".\n",
      "191115-08:22:25,571 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.brain_extraction.brain_extraction_per_session_visit_id_deiter\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/brain_extraction_per_session_visit_id_deiter\".\n",
      "191115-08:22:25,580 nipype.workflow INFO:\n",
      "\t [Node] Running \"brain_extraction_per_session_visit_id_deiter\" (\"nipype.interfaces.utility.base.IdentityInterface\")\n",
      "191115-08:22:25,592 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.brain_extraction.brain_extraction_per_session_visit_id_deiter\".\n",
      "191115-08:22:25,594 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.brain_extraction.brain_extraction_final\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/brain_extraction/brain_extraction_final\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191115-08:22:25,603 nipype.workflow INFO:\n",
      "\t [Node] Running \"brain_extraction_final\" (\"nipype.interfaces.utility.base.Merge\")\n",
      "191115-08:22:25,615 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.brain_extraction.brain_extraction_final\".\n",
      "191115-08:22:25,618 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_prereqs\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/smooth_mask_prereqs\".\n",
      "191115-08:22:25,629 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_prereqs\" (\"nipype.interfaces.utility.base.Merge\")\n",
      "191115-08:22:25,639 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_prereqs\".\n",
      "191115-08:22:25,641 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_per_session_source\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub1/_visit_id_VISIT/smooth_mask_per_session_source\".\n",
      "191115-08:22:25,660 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_per_session_source\" (\"arcana.repository.interfaces.RepositorySource\")\n",
      "191115-08:22:25,700 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_per_session_source\".\n",
      "191115-08:22:25,702 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_smooth\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub1/_visit_id_VISIT/smooth_mask_smooth\".\n",
      "191115-08:22:25,716 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_smooth\" (\"nipype.interfaces.fsl.maths.IsotropicSmooth\"), a CommandLine Interface with command:\n",
      "fslmaths /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth1/sub1/sub-01_ses-test_T1w.nii.gz -s 1.69864 /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub1/_visit_id_VISIT/smooth_mask_smooth/sub-01_ses-test_T1w_smooth.nii.gz\n",
      "191115-08:22:30,543 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_smooth\".\n",
      "191115-08:22:30,545 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_conv_smooth_from_nifti_gz_format\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub1/_visit_id_VISIT/smooth_mask_conv_smooth_from_nifti_gz_format\".\n",
      "191115-08:22:30,554 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_conv_smooth_from_nifti_gz_format\" (\"banana.interfaces.mrtrix.utils.MRConvert\"), a CommandLine Interface with command:\n",
      "mrconvert -quiet /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub1/_visit_id_VISIT/smooth_mask_smooth/sub-01_ses-test_T1w_smooth.nii.gz /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub1/_visit_id_VISIT/smooth_mask_conv_smooth_from_nifti_gz_format/sub-01_ses-test_T1w_smooth_conv.mif\n",
      "191115-08:22:31,223 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_conv_smooth_from_nifti_gz_format\".\n",
      "191115-08:22:31,225 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_mask\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub1/_visit_id_VISIT/smooth_mask_mask\".\n",
      "191115-08:22:31,233 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_mask\" (\"nipype.interfaces.fsl.maths.ApplyMask\"), a CommandLine Interface with command:\n",
      "fslmaths /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub1/_visit_id_VISIT/smooth_mask_smooth/sub-01_ses-test_T1w_smooth.nii.gz -mas /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth1/sub1/my_extended_analysis/brain_mask.nii.gz /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub1/_visit_id_VISIT/smooth_mask_mask/sub-01_ses-test_T1w_smooth_masked.nii.gz\n",
      "191115-08:22:32,302 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_mask\".\n",
      "191115-08:22:32,303 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_per_session_sink\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub1/_visit_id_VISIT/smooth_mask_per_session_sink\".\n",
      "191115-08:22:32,318 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_per_session_sink\" (\"arcana.repository.interfaces.RepositorySink\")\n",
      "191115-08:22:32,492 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_per_session_sink\".\n",
      "191115-08:22:32,493 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_per_session_source\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub3/_visit_id_VISIT/smooth_mask_per_session_source\".\n",
      "191115-08:22:32,505 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_per_session_source\" (\"arcana.repository.interfaces.RepositorySource\")\n",
      "191115-08:22:32,539 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_per_session_source\".\n",
      "191115-08:22:32,540 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_smooth\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub3/_visit_id_VISIT/smooth_mask_smooth\".\n",
      "191115-08:22:32,550 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_smooth\" (\"nipype.interfaces.fsl.maths.IsotropicSmooth\"), a CommandLine Interface with command:\n",
      "fslmaths /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth1/sub3/sub-03_ses-test_T1w.nii.gz -s 1.69864 /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub3/_visit_id_VISIT/smooth_mask_smooth/sub-03_ses-test_T1w_smooth.nii.gz\n",
      "191115-08:22:37,194 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_smooth\".\n",
      "191115-08:22:37,196 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_conv_smooth_from_nifti_gz_format\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub3/_visit_id_VISIT/smooth_mask_conv_smooth_from_nifti_gz_format\".\n",
      "191115-08:22:37,203 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_conv_smooth_from_nifti_gz_format\" (\"banana.interfaces.mrtrix.utils.MRConvert\"), a CommandLine Interface with command:\n",
      "mrconvert -quiet /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub3/_visit_id_VISIT/smooth_mask_smooth/sub-03_ses-test_T1w_smooth.nii.gz /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub3/_visit_id_VISIT/smooth_mask_conv_smooth_from_nifti_gz_format/sub-03_ses-test_T1w_smooth_conv.mif\n",
      "191115-08:22:37,559 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_conv_smooth_from_nifti_gz_format\".\n",
      "191115-08:22:37,561 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_mask\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub3/_visit_id_VISIT/smooth_mask_mask\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191115-08:22:37,569 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_mask\" (\"nipype.interfaces.fsl.maths.ApplyMask\"), a CommandLine Interface with command:\n",
      "fslmaths /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub3/_visit_id_VISIT/smooth_mask_smooth/sub-03_ses-test_T1w_smooth.nii.gz -mas /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth1/sub3/my_extended_analysis/brain_mask.nii.gz /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub3/_visit_id_VISIT/smooth_mask_mask/sub-03_ses-test_T1w_smooth_masked.nii.gz\n",
      "191115-08:22:38,541 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_mask\".\n",
      "191115-08:22:38,542 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_per_session_sink\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub3/_visit_id_VISIT/smooth_mask_per_session_sink\".\n",
      "191115-08:22:38,556 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_per_session_sink\" (\"arcana.repository.interfaces.RepositorySink\")\n",
      "191115-08:22:38,727 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_per_session_sink\".\n",
      "191115-08:22:38,728 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_per_session_source\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub2/_visit_id_VISIT/smooth_mask_per_session_source\".\n",
      "191115-08:22:38,737 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_per_session_source\" (\"arcana.repository.interfaces.RepositorySource\")\n",
      "191115-08:22:38,763 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_per_session_source\".\n",
      "191115-08:22:38,764 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_smooth\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub2/_visit_id_VISIT/smooth_mask_smooth\".\n",
      "191115-08:22:38,769 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_smooth\" (\"nipype.interfaces.fsl.maths.IsotropicSmooth\"), a CommandLine Interface with command:\n",
      "fslmaths /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth1/sub2/sub-02_ses-test_T1w.nii.gz -s 1.69864 /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub2/_visit_id_VISIT/smooth_mask_smooth/sub-02_ses-test_T1w_smooth.nii.gz\n",
      "191115-08:22:43,268 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_smooth\".\n",
      "191115-08:22:43,269 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_conv_smooth_from_nifti_gz_format\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub2/_visit_id_VISIT/smooth_mask_conv_smooth_from_nifti_gz_format\".\n",
      "191115-08:22:43,276 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_conv_smooth_from_nifti_gz_format\" (\"banana.interfaces.mrtrix.utils.MRConvert\"), a CommandLine Interface with command:\n",
      "mrconvert -quiet /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub2/_visit_id_VISIT/smooth_mask_smooth/sub-02_ses-test_T1w_smooth.nii.gz /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub2/_visit_id_VISIT/smooth_mask_conv_smooth_from_nifti_gz_format/sub-02_ses-test_T1w_smooth_conv.mif\n",
      "191115-08:22:43,665 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_conv_smooth_from_nifti_gz_format\".\n",
      "191115-08:22:43,666 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_mask\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub2/_visit_id_VISIT/smooth_mask_mask\".\n",
      "191115-08:22:43,674 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_mask\" (\"nipype.interfaces.fsl.maths.ApplyMask\"), a CommandLine Interface with command:\n",
      "fslmaths /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub2/_visit_id_VISIT/smooth_mask_smooth/sub-02_ses-test_T1w_smooth.nii.gz -mas /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth1/sub2/my_extended_analysis/brain_mask.nii.gz /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub2/_visit_id_VISIT/smooth_mask_mask/sub-02_ses-test_T1w_smooth_masked.nii.gz\n",
      "191115-08:22:44,666 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_mask\".\n",
      "191115-08:22:44,667 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_per_session_sink\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_subject_id_sub2/_visit_id_VISIT/smooth_mask_per_session_sink\".\n",
      "191115-08:22:44,681 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_per_session_sink\" (\"arcana.repository.interfaces.RepositorySink\")\n",
      "191115-08:22:44,838 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_per_session_sink\".\n",
      "191115-08:22:44,839 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_per_session_subject_id_deiter\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/_visit_id_VISIT/smooth_mask_per_session_subject_id_deiter\".\n",
      "191115-08:22:44,844 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_per_session_subject_id_deiter\" (\"nipype.interfaces.utility.base.IdentityInterface\")\n",
      "191115-08:22:44,851 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_per_session_subject_id_deiter\".\n",
      "191115-08:22:44,852 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_per_session_visit_id_deiter\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/smooth_mask_per_session_visit_id_deiter\".\n",
      "191115-08:22:44,856 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_per_session_visit_id_deiter\" (\"nipype.interfaces.utility.base.IdentityInterface\")\n",
      "191115-08:22:44,862 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_per_session_visit_id_deiter\".\n",
      "191115-08:22:44,863 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.smooth_mask.smooth_mask_final\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/smooth_mask/smooth_mask_final\".\n",
      "191115-08:22:44,872 nipype.workflow INFO:\n",
      "\t [Node] Running \"smooth_mask_final\" (\"nipype.interfaces.utility.base.Merge\")\n",
      "191115-08:22:44,880 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.smooth_mask.smooth_mask_final\".\n",
      "191115-08:22:44,881 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.image_std_pipeline.image_std_pipeline_prereqs\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/image_std_pipeline/image_std_pipeline_prereqs\".\n",
      "191115-08:22:44,888 nipype.workflow INFO:\n",
      "\t [Node] Running \"image_std_pipeline_prereqs\" (\"nipype.interfaces.utility.base.Merge\")\n",
      "191115-08:22:44,895 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.image_std_pipeline.image_std_pipeline_prereqs\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191115-08:22:44,896 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_source\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/image_std_pipeline/_subject_id_sub1/_visit_id_VISIT/image_std_pipeline_per_session_source\".\n",
      "191115-08:22:44,909 nipype.workflow INFO:\n",
      "\t [Node] Running \"image_std_pipeline_per_session_source\" (\"arcana.repository.interfaces.RepositorySource\")\n",
      "191115-08:22:44,923 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_source\".\n",
      "191115-08:22:44,924 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.image_std_pipeline.image_std_pipeline_mask\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/image_std_pipeline/_subject_id_sub1/_visit_id_VISIT/image_std_pipeline_mask\".\n",
      "191115-08:22:44,929 nipype.workflow INFO:\n",
      "\t [Node] Running \"image_std_pipeline_mask\" (\"nipype.interfaces.fsl.utils.ImageStats\"), a CommandLine Interface with command:\n",
      "fslstats /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth1/sub1/my_extended_analysis/smooth_masked.nii.gz -s\n",
      "191115-08:22:45,192 nipype.interface INFO:\n",
      "\t stdout 2019-11-15T08:22:45.192591:126.144342 \n",
      "191115-08:22:45,271 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.image_std_pipeline.image_std_pipeline_mask\".\n",
      "191115-08:22:45,272 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_sink\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/image_std_pipeline/_subject_id_sub1/_visit_id_VISIT/image_std_pipeline_per_session_sink\".\n",
      "191115-08:22:45,285 nipype.workflow INFO:\n",
      "\t [Node] Running \"image_std_pipeline_per_session_sink\" (\"arcana.repository.interfaces.RepositorySink\")\n",
      "191115-08:22:45,294 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_sink\".\n",
      "191115-08:22:45,295 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_source\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/image_std_pipeline/_subject_id_sub3/_visit_id_VISIT/image_std_pipeline_per_session_source\".\n",
      "191115-08:22:45,305 nipype.workflow INFO:\n",
      "\t [Node] Running \"image_std_pipeline_per_session_source\" (\"arcana.repository.interfaces.RepositorySource\")\n",
      "191115-08:22:45,322 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_source\".\n",
      "191115-08:22:45,323 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.image_std_pipeline.image_std_pipeline_mask\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/image_std_pipeline/_subject_id_sub3/_visit_id_VISIT/image_std_pipeline_mask\".\n",
      "191115-08:22:45,328 nipype.workflow INFO:\n",
      "\t [Node] Running \"image_std_pipeline_mask\" (\"nipype.interfaces.fsl.utils.ImageStats\"), a CommandLine Interface with command:\n",
      "fslstats /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth1/sub3/my_extended_analysis/smooth_masked.nii.gz -s\n",
      "191115-08:22:45,594 nipype.interface INFO:\n",
      "\t stdout 2019-11-15T08:22:45.594260:152.161282 \n",
      "191115-08:22:45,677 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.image_std_pipeline.image_std_pipeline_mask\".\n",
      "191115-08:22:45,678 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_sink\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/image_std_pipeline/_subject_id_sub3/_visit_id_VISIT/image_std_pipeline_per_session_sink\".\n",
      "191115-08:22:45,692 nipype.workflow INFO:\n",
      "\t [Node] Running \"image_std_pipeline_per_session_sink\" (\"arcana.repository.interfaces.RepositorySink\")\n",
      "191115-08:22:45,701 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_sink\".\n",
      "191115-08:22:45,702 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_source\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/image_std_pipeline/_subject_id_sub2/_visit_id_VISIT/image_std_pipeline_per_session_source\".\n",
      "191115-08:22:45,713 nipype.workflow INFO:\n",
      "\t [Node] Running \"image_std_pipeline_per_session_source\" (\"arcana.repository.interfaces.RepositorySource\")\n",
      "191115-08:22:45,729 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_source\".\n",
      "191115-08:22:45,730 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.image_std_pipeline.image_std_pipeline_mask\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/image_std_pipeline/_subject_id_sub2/_visit_id_VISIT/image_std_pipeline_mask\".\n",
      "191115-08:22:45,735 nipype.workflow INFO:\n",
      "\t [Node] Running \"image_std_pipeline_mask\" (\"nipype.interfaces.fsl.utils.ImageStats\"), a CommandLine Interface with command:\n",
      "fslstats /Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/output/sample-datasets/depth1/sub2/my_extended_analysis/smooth_masked.nii.gz -s\n",
      "191115-08:22:45,995 nipype.interface INFO:\n",
      "\t stdout 2019-11-15T08:22:45.995301:140.465662 \n",
      "191115-08:22:46,79 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.image_std_pipeline.image_std_pipeline_mask\".\n",
      "191115-08:22:46,80 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_sink\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/image_std_pipeline/_subject_id_sub2/_visit_id_VISIT/image_std_pipeline_per_session_sink\".\n",
      "191115-08:22:46,93 nipype.workflow INFO:\n",
      "\t [Node] Running \"image_std_pipeline_per_session_sink\" (\"arcana.repository.interfaces.RepositorySink\")\n",
      "191115-08:22:46,102 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_sink\".\n",
      "191115-08:22:46,103 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_subject_id_deiter\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/image_std_pipeline/_visit_id_VISIT/image_std_pipeline_per_session_subject_id_deiter\".\n",
      "191115-08:22:46,109 nipype.workflow INFO:\n",
      "\t [Node] Running \"image_std_pipeline_per_session_subject_id_deiter\" (\"nipype.interfaces.utility.base.IdentityInterface\")\n",
      "191115-08:22:46,116 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_subject_id_deiter\".\n",
      "191115-08:22:46,117 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_visit_id_deiter\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/image_std_pipeline/image_std_pipeline_per_session_visit_id_deiter\".\n",
      "191115-08:22:46,122 nipype.workflow INFO:\n",
      "\t [Node] Running \"image_std_pipeline_per_session_visit_id_deiter\" (\"nipype.interfaces.utility.base.IdentityInterface\")\n",
      "191115-08:22:46,129 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.image_std_pipeline.image_std_pipeline_per_session_visit_id_deiter\".\n",
      "191115-08:22:46,130 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"image_std_pipeline.image_std_pipeline.image_std_pipeline_final\" in \"/Users/tclose/Documents/Workshops/2019-11-15-N.A.B.-workshop/nipype_arcana_workshop/notebooks/work/image_std_pipeline/image_std_pipeline/image_std_pipeline_final\".\n",
      "191115-08:22:46,135 nipype.workflow INFO:\n",
      "\t [Node] Running \"image_std_pipeline_final\" (\"nipype.interfaces.utility.base.Merge\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191115-08:22:46,142 nipype.workflow INFO:\n",
      "\t [Node] Finished \"image_std_pipeline.image_std_pipeline.image_std_pipeline_final\".\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f940e7411862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image_std'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Subject/visit ({}/{}): {} '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "from arcana import Dataset, FilesetFilter\n",
    "\n",
    "\n",
    "my_analysis = MyExtendedBasicBrainAnalysis(\n",
    "    'my_extended_analysis',  # The name needs to be the same as the previous version\n",
    "    dataset=Dataset('output/sample-datasets/depth1', depth=1),\n",
    "    processor='work',\n",
    "    inputs=[\n",
    "        FilesetFilter('magnitude', '.*T1w$', is_regex=True)])\n",
    "\n",
    "my_analysis.derive('image_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject/visit (sub1/VISIT): 126.144342 \n",
      "Subject/visit (sub2/VISIT): 140.465662 \n",
      "Subject/visit (sub3/VISIT): 152.161282 \n"
     ]
    }
   ],
   "source": [
    "for std in my_analysis.data('image_std'):\n",
    "    print('Subject/visit ({}/{}): {} '.format(std.subject_id, std.visit_id, std.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
