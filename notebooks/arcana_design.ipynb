{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Analysis Classes\n",
    "\n",
    "We will now learn how to create your own Analysis class by going through the implementation of the `BasicBrainAnalysis` class from the \"Applying Analysis Classes\" notebook.\n",
    "\n",
    "There are three key components of an Analysis class:\n",
    "\n",
    "* Data specification\n",
    "* Parameter specification\n",
    "* Pipeline constructor methods\n",
    "\n",
    "The data specification defines all inputs, outputs and intermediate derivatives. The parameter specification specifies the free (meta) parameters that can be used to customise the analysis. Pipeline constructor methods are just regular methods of the class that return a `Pipeline` that generates one or more derivatives.\n",
    "\n",
    "## Base and Meta-classes and Inheritance\n",
    "\n",
    "Every Analysis class should (at least indirectly) inherit from the `arcana.Analysis` base class. This contains methods to perform a lot of the \"magic\" that Arcana does. \n",
    "\n",
    "Arcana is designed to utilise inheritance of Analysis classes, but since Analysis classes specify a number of class attributes (i.e. for the data and parameter specifications) all Analysis classes need to be constructed by a special \"meta-class\", `arcana.AnalysisMetaClass`. However, you don't need to understand what is going on behind the scenes (or what a meta-class is even), just simply define your class like this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcana import Analysis, AnalysisMetaClass\n",
    "\n",
    "class MyBasicBrainAnalysis(Analysis, metaclass=AnalysisMetaClass):\n",
    "    \n",
    "    # Stuff goes here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data specification\n",
    "\n",
    "The data specification is the place to start designing an Analysis class. As the name suggests, it specifies all inputs, outputs and intermediate derivatives of the class via a list of \"data-spec\" objects:\n",
    "\n",
    "* `FilesetSpec` (intermediate file-set derivatives)\n",
    "* `InputFilesetSpec`\n",
    "* `OutputFilesetSpec`\n",
    "* `FieldSpec` (intermediate field derivatives)\n",
    "* `InputFieldSpec`\n",
    "* `OutputFieldSpec`\n",
    "\n",
    "Instead of setting the data specification directly, data-spec objects are appended to the specifications of base classes (by the meta-class) by defining the `add_data_specs` class attribute. This enables the data specifications of base classes to be altered and overwritten.\n",
    "\n",
    "Each data-spec object is given a name (i.e. the one that appears on the class \"menu\") and assigned a file-format (filesets) or data type (fields). The key difference between input and output (and intermediate) data-specs is that output data-specs refer to the \"pipeline constructor method\" that constructs a pipeline to generate them. Intermediate specs are equivalent to output specs in every respect except they appear don't appear on the class menu by default.\n",
    "\n",
    "Typically, the only thing you will ever need to do with a data-spec is initialise it and append it to the `add_data_specs` list, and the best place to see the available initialisation options is the doc strings, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from arcana import InputFilesetSpec, InputFieldSpec, FilesetSpec, FieldSpec\n",
    "print('InputFilesetSpec:\\n', InputFilesetSpec.__doc__)\n",
    "print('InputFieldSpec:\\n', InputFieldSpec.__doc__)\n",
    "print('FilesetSpec:\\n', FilesetSpec.__doc__)\n",
    "print('FieldSpec:\\n', FieldSpec.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key parameters to note are:\n",
    "\n",
    "#### file_format/datatype\n",
    "\n",
    "The format/data-type that the item will be converted to when it is stored in the dataset\n",
    "\n",
    "#### frequency\n",
    "\n",
    "Where the \"data-items\" sit in the dataset tree, i.e. whether there is one for every session, subject, visit or the whole dataset. Valid values for `frequency` are\n",
    "\n",
    "* 'per_session'\n",
    "* 'per_subject'\n",
    "* 'per_visit'\n",
    "* 'per_dataset'\n",
    "\n",
    "#### pipeline_getter\n",
    "\n",
    "The name of the method in the class that constructs the pipeline to generate the derivatives\n",
    "\n",
    "### Example\n",
    "\n",
    "In the Basic-Brain Analysis class we define three output, one input and one intermediate fileset specs as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcana import InputFilesetSpec, FilesetSpec, OutputFilesetSpec\n",
    "from banana.file_format import nifti_gz_format\n",
    "\n",
    "class MyBasicBrainAnalysis(Analysis, metaclass=AnalysisMetaClass):\n",
    "\n",
    "    add_data_specs = [\n",
    "        InputFilesetSpec('magnitude', nifti_gz_format,\n",
    "                         desc=\"A magnitude image (e.g. T1w, T2w, etc..)\"),\n",
    "        OutputFilesetSpec('brain', nifti_gz_format,\n",
    "                          'brain_extraction_pipeline',\n",
    "                          desc=\"Skull-stripped magnitude image\"),\n",
    "        FilesetSpec('brain_mask', nifti_gz_format,\n",
    "                    'brain_extraction_pipeline',\n",
    "                    desc=\"Brain mask used for skull-stripping\"),\n",
    "        OutputFilesetSpec('smooth', nifti_gz_format, 'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed magnitude image\"),\n",
    "        OutputFilesetSpec('smooth_masked', nifti_gz_format,\n",
    "                          'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed and masked magnitude image\")]\n",
    "    \n",
    "    def brain_extraction_pipeline(self, **name_maps):\n",
    "        \"We'll define this later\"\n",
    "    \n",
    "    def smooth_mask_pipeline(self, **name_maps):\n",
    "        \"We'll define this later\"\n",
    "\n",
    "print(MyBasicBrainAnalysis.static_menu(full=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** how the 'pipeline_getter' parameters of the spec objects reference the name of a \"pipeline constructor\" method defined in the class. The matches between these names are checked by the metaclass so if we don't define them Arcana will throw an error. \n",
    "\n",
    "Since `MyBasicBrainAnalysis` inherits directly from `Analysis` the only specs in the data specification are those in `add_data_specs`. However, if we would like to extend `MyBasicBrainAnalysis` to make a new class `MyExtendedBasicBrainAnalysis` we can add to and override the specs from `MyBasicBrainAnalysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from banana.file_format import mrtrix_image_format\n",
    "\n",
    "class MyExtendedBasicBrainAnalysis(MyBasicBrainAnalysis, metaclass=AnalysisMetaClass):\n",
    "\n",
    "    add_data_specs = [\n",
    "        OutputFilesetSpec('smooth', mrtrix_image_format, 'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed magnitude image in Mrtrix format\"),\n",
    "        OutputFilesetSpec('thresholded', nifti_gz_format,\n",
    "                          'threshold_pipeline',\n",
    "                          desc=\"Thresholded smoothed magnitude image\")]\n",
    "    \n",
    "    def threshold_pipeline(self, **name_maps):\n",
    "        \"We'll define this later\"\n",
    "        \n",
    "print(MyExtendedBasicBrainAnalysis.static_menu(full=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is now a `thresholded` output, and the `smooth` image is now of `mrtrix_image` format instead of `nifti_gz`. \n",
    "\n",
    "**Note**: we can get away with changing the format of the `smooth` from zipped NiFTI to MRtrix because if the output format of the pipeline doesn't match the format of the specification it will be automatically converted before it is stored in the dataset (as long as a converter exists)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Specification\n",
    "\n",
    "The parameter specification works very much like the data specification but for parameters. \"Param-specs\" can be either of class `ParamSpec` or `SwitchSpec` type.\n",
    "\n",
    "`SwitchSpec`s are used to qualitatively change the analysis performed, e.g. using FSL for non-linear registration to a template (i.e. FNIRT) instead of ANTs. `ParamSpec`s are used to quantitatively change the analysis, e.g. change the required threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcana import ParamSpec, SwitchSpec\n",
    "\n",
    "print('ParamSpec:\\n', ParamSpec.__doc__)\n",
    "print('SwitchSpec:\\n', SwitchSpec.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the data specification, instead of setting the parameter specification directly it is added to the class via `add_param_specs` to allow manipulation by subclasses.\n",
    "\n",
    "Returning to the `BasicBrainAnalysis` example we add in the FWHM parameter used in the smoothing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBrainAnalysis(Analysis, metaclass=AnalysisMetaClass):\n",
    "    \"\"\"\n",
    "    A baisc example that demonstrates how Analysis classes work.\n",
    "    \"\"\"\n",
    "\n",
    "    add_data_specs = [\n",
    "        InputFilesetSpec('magnitude', nifti_gz_format,\n",
    "                         desc=\"A magnitude image (e.g. T1w, T2w, etc..)\"),\n",
    "        OutputFilesetSpec('brain', nifti_gz_format,\n",
    "                          'brain_extraction_pipeline',\n",
    "                          desc=\"Skull-stripped magnitude image\"),\n",
    "        FilesetSpec('brain_mask', nifti_gz_format,\n",
    "                    'brain_extraction_pipeline',\n",
    "                    desc=\"Brain mask used for skull-stripping\"),\n",
    "        OutputFilesetSpec('smooth', nifti_gz_format, 'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed magnitude image\"),\n",
    "        OutputFilesetSpec('smooth_masked', nifti_gz_format,\n",
    "                          'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed and masked magnitude image\")]\n",
    "\n",
    "    add_param_specs = [\n",
    "        ParamSpec('smoothing_fwhm', 4.0,\n",
    "                  desc=(\"The full-width-half-maxium radius of the smoothing \"\n",
    "                        \"kernel\"))]\n",
    "\n",
    "    def brain_extraction_pipeline(self, **name_maps):\n",
    "        \"We'll define this later\"\n",
    "    \n",
    "    def smooth_mask_pipeline(self, **name_maps):\n",
    "        \"We'll define this later\"\n",
    "\n",
    "print(BasicBrainAnalysis.static_menu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Constructor Methods\n",
    "\n",
    "Pipeline constructor methods are where the action happens in Analysis classes. They return `Pipeline` objects (which are just thin wrappers around `nipype.Workflows`) that link the data specification together by taking one or more data-specs as inputs and generating one or more as outputs.\n",
    "\n",
    "### Initialising a pipeline\n",
    "\n",
    "The basic form of a pipeline constructor method is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from banana.citation import fsl_cite\n",
    "\n",
    "def smooth_mask_pipeline(self, **name_maps):\n",
    "\n",
    "    pipeline = self.new_pipeline(\n",
    "        'smooth_mask',\n",
    "        desc=\"Smooths and masks a brain image\",\n",
    "        name_maps=name_maps,\n",
    "        citations=[fsl_cite])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the `new_pipeline` creates the `Pipeline` object, which is returned at the end of the method. Looking at the doc string of the `new_pipeline` we can see the parameters that you need/can pass to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Analysis.new_pipeline.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `name` parameter is just used internally to distinguish working directories between pipeline nodes. There aren't any requirements on it except that needs to be unique amongst all pipelines that can be generated by the Analysis instance.\n",
    "\n",
    "`citations` lists the publications that should be cited when using this pipeline. However, I plan to replace this bespoke solution with the third-party package [duecredit](https://pypi.org/project/duecredit/), which nipype uses.\n",
    "\n",
    "The `name_maps` parameter should be passed directly from the keyword arguments of the pipeline constructor. It allows sub-classes to do funky things when manipulating methods defined in base classes such as mapping inputs and outputs onto different data-specs. However, going into the details of how it works is beyond the scope of this notebook.\n",
    "\n",
    "### Adding nodes to a pipeline\n",
    "\n",
    "The syntax for adding nodes to a pipeline is somewhat different to how it is done in Nipype. This is because it is modelled on the proposed syntax for [Nipype v2.0](https://github.com/nipy/nipype/projects/8), which aims to streamline code for workflow construction and make it easy to read.\n",
    "\n",
    "Nodes are added to a pipeline using the `add` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcana.pipeline import Pipeline\n",
    "print(Pipeline.add.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which in our BasicBrainAnalysis example looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_mask_pipeline(self, **name_maps):\n",
    "\n",
    "    pipeline = self.new_pipeline(\n",
    "        'smooth_mask',\n",
    "        desc=\"Smooths and masks a brain image\",\n",
    "        name_maps=name_maps,\n",
    "        citations=[fsl_cite])\n",
    "\n",
    "    # Smoothing process\n",
    "    smooth = pipeline.add(\n",
    "        'smooth',\n",
    "        fsl.IsotropicSmooth(\n",
    "            fwhm=self.parameter('smoothing_fwhm')),           # Param. passed from param-spec\n",
    "        inputs={\n",
    "            'in_file': ('magnitude', nifti_gz_format)},       # Input from data-spec\n",
    "        outputs={\n",
    "            'smooth': ('out_file', nifti_gz_format)},         # Output to data-spec\n",
    "        requirements=[\n",
    "            fsl_req.v('5.0.10')])                             # Requires FSL >= 5.0.10\n",
    "    \n",
    "    pipeline.add(\n",
    "        'mask',\n",
    "        fsl.ApplyMask(\n",
    "            output_datatype=int),                             # Fixed param of pipeline\n",
    "        inputs={\n",
    "            'in_file': (smooth, 'out_file'),                  # Input from previous node\n",
    "            'mask_file': ('brain_mask', nifti_gz_format)},    # Input from data-spec\n",
    "        outputs={\n",
    "            'smooth_masked': ('out_file', nifti_gz_format)},  # Output to data-spec\n",
    "        requirements=[\n",
    "            fsl_req.v('5.0.10')])                             # Requires FSL >= 5.0.10\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key points to note are:\n",
    "\n",
    "* All nodes need a unique name within the pipeline (as in Nipype)\n",
    "* Stylistic convention dictates that constant interface traits are set when the interface is initialised\n",
    "* `inputs` is a dictionary that maps the name of an input-trait of the interface to a 2-tuple consisting of either\n",
    "  * a data-spec name (i.e. inputs of the pipeline) and the file-format/datatype the input is expected in (the format will be automatically converted if required)\n",
    "  * an upstream node and name of the trait to connect from the upstream node\n",
    "* `outputs` is a dictionary that maps data-spec names (i.e. outputs of the pipeline) to a 2-tuple consisting of the name of an output-trait and the file-format/datatype it is produced in.\n",
    "* `requirements` are a list of `arcana.Requirement` objects that specify versions of external packages (e.g. FSL, SPM, MRtrix) that are required for the node to run.\n",
    "\n",
    "### Merging and Splitting Pipelines Across Subjects/Visits\n",
    "\n",
    "Arcana handles iteration over subjects and sessions in the background as implicitly specifed by the frequencies of inputs and outputs of the pipeline. However, in some cases you may need to join over all subjects/visits to create a summary statistic (e.g. mean), and then potentially use this variable back on an individual subject/visit level again (e.g. normalisation). As we saw in the in the morning \"Advanced Nipype\" section, these cases are handled by Nipype using iterators, map nodes and join nodes.\n",
    "\n",
    "In Arcana join nodes are specified by providing the `joinsource` an `joinfield` parameters of when adding a node to a pipeline. These parameters work the same as they do for Nipype map nodes. Access to Arcana's implicit iterator nodes are exposed via the `self.SUBJECT_ID` and `self.VISIT_ID` variables. For example, in the `statistics_pipeline` of the `example.analysis.ToyAnalysis` we do a two-step merge, first over visits and then subjects to create the *per_dataset* metrics 'average' and 'std_dev' from the *per_session* 'selected_metric'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_pipeline(self, **name_maps):\n",
    "    pipeline = self.new_pipeline(\n",
    "        name='statistics',\n",
    "        name_maps=name_maps,\n",
    "        desc=\"Calculate statistics\")\n",
    "\n",
    "    merge_visits = pipeline.add(\n",
    "        'merge_visits',\n",
    "        Merge(\n",
    "            numinputs=1),\n",
    "        inputs={\n",
    "            'in1': ('selected_metric', text_format)},\n",
    "        joinsource=self.VISIT_ID,\n",
    "        joinfield=['in1'])\n",
    "\n",
    "    merge_subjects = pipeline.add(\n",
    "        'merge_subjects',\n",
    "        Merge(\n",
    "            numinputs=1,\n",
    "            ravel_inputs=True),\n",
    "        inputs={\n",
    "            'in1': (merge_visits, 'out')},\n",
    "        joinsource=self.SUBJECT_ID,\n",
    "        joinfield=['in1'])\n",
    "\n",
    "    concat = pipeline.add(\n",
    "        'concat',\n",
    "        ConcatFloats(),\n",
    "        inputs={\n",
    "            'in_files': (merge_subjects, 'out')})\n",
    "\n",
    "    pipeline.add(\n",
    "        'extract_metrics',\n",
    "        ExtractMetrics(),\n",
    "        inputs={\n",
    "            'in_list': (concat, 'out_list')},\n",
    "        outputs={\n",
    "            'average': ('avg', float),\n",
    "            'std_dev': ('std', float)})\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`self.SUBJECT_ID` and `self.VISIT_ID` can also be used to expand over all subject/visits again and to create a map node, simply provide the `iterfield` parameter when adding a node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "We can now put the three key components together (i.e. data & parameter specifications and pipeline constructor methods) to make a fully-functioning Analysis class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBrainAnalysis(Analysis, metaclass=AnalysisMetaClass):\n",
    "    \"\"\"\n",
    "    A baisc analysis class that demonstrates how Analysis classes work.\n",
    "    \"\"\"\n",
    "\n",
    "    add_data_specs = [\n",
    "        InputFilesetSpec('magnitude', nifti_gz_format,\n",
    "                         desc=\"A magnitude image (e.g. T1w, T2w, etc..)\"),\n",
    "        OutputFilesetSpec('brain', nifti_gz_format,\n",
    "                          'brain_extraction_pipeline',\n",
    "                          desc=\"Skull-stripped magnitude image\"),\n",
    "        FilesetSpec('brain_mask', nifti_gz_format,\n",
    "                    'brain_extraction_pipeline',\n",
    "                    desc=\"Brain mask used for skull-stripping\"),\n",
    "        OutputFilesetSpec('smooth', nifti_gz_format, 'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed magnitude image\"),\n",
    "        OutputFilesetSpec('smooth_masked', nifti_gz_format,\n",
    "                          'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed and masked magnitude image\")]\n",
    "\n",
    "    add_param_specs = [\n",
    "        ParamSpec('smoothing_fwhm', 4.0,\n",
    "                  desc=(\"The full-width-half-maxium radius of the smoothing \"\n",
    "                        \"kernel\"))]\n",
    "\n",
    "    def brain_extraction_pipeline(self, **name_maps):\n",
    "\n",
    "        pipeline = self.new_pipeline(\n",
    "            'brain_extraction',\n",
    "            desc=\"Extracts brain from full-head image\",\n",
    "            name_maps=name_maps,\n",
    "            citations=[fsl_cite])\n",
    "\n",
    "        pipeline.add(\n",
    "            'bet',\n",
    "            fsl.BET(\n",
    "                mask=True),\n",
    "            inputs={\n",
    "                'in_file': ('magnitude', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'brain': ('out_file', nifti_gz_format),\n",
    "                'brain_mask': ('mask_file', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        return pipeline\n",
    "\n",
    "    def smooth_mask_pipeline(self, **name_maps):\n",
    "\n",
    "        pipeline = self.new_pipeline(\n",
    "            'smooth_mask',\n",
    "            desc=\"Smooths and masks a brain image\",\n",
    "            name_maps=name_maps,\n",
    "            citations=[fsl_cite])\n",
    "\n",
    "        # Smoothing process\n",
    "        smooth = pipeline.add(\n",
    "            'smooth',\n",
    "            fsl.IsotropicSmooth(\n",
    "                fwhm=self.parameter('smoothing_fwhm')),\n",
    "            inputs={\n",
    "                'in_file': ('magnitude', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'smooth': ('out_file', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        pipeline.add(\n",
    "            'mask',\n",
    "            fsl.ApplyMask(),\n",
    "            inputs={\n",
    "                'in_file': (smooth, 'out_file'),\n",
    "                'mask_file': ('brain_mask', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'smooth_masked': ('out_file', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to Create Publication Outputs\n",
    "\n",
    "While not necessary, if you are creating a new Analysis class for your specific study, it is a nice idea to implement additional methods to generate all your publication outputs (figures, stats, etc...) within the Analysis class.\n",
    "\n",
    "For example in the `BasicBrainAnalysis` class we have the method `plot_comparison`, which is implemented as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BasicBrainAnalysis(Analysis, metaclass=AnalysisMetaClass):\n",
    "    \"\"\"\n",
    "    A baisc analysis class that demonstrates how Analysis classes work.\n",
    "    \"\"\"\n",
    "\n",
    "    add_data_specs = [\n",
    "        InputFilesetSpec('magnitude', nifti_gz_format,\n",
    "                         desc=\"A magnitude image (e.g. T1w, T2w, etc..)\"),\n",
    "        OutputFilesetSpec('brain', nifti_gz_format,\n",
    "                          'brain_extraction_pipeline',\n",
    "                          desc=\"Skull-stripped magnitude image\"),\n",
    "        FilesetSpec('brain_mask', nifti_gz_format,\n",
    "                    'brain_extraction_pipeline',\n",
    "                    desc=\"Brain mask used for skull-stripping\"),\n",
    "        OutputFilesetSpec('smooth', nifti_gz_format, 'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed magnitude image\"),\n",
    "        OutputFilesetSpec('smooth_masked', nifti_gz_format,\n",
    "                          'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed and masked magnitude image\")]\n",
    "\n",
    "    add_param_specs = [\n",
    "        ParamSpec('smoothing_fwhm', 4.0,\n",
    "                  desc=(\"The full-width-half-maxium radius of the smoothing \"\n",
    "                        \"kernel\"))]\n",
    "\n",
    "    def brain_extraction_pipeline(self, **name_maps):\n",
    "\n",
    "        pipeline = self.new_pipeline(\n",
    "            'brain_extraction',\n",
    "            desc=\"Extracts brain from full-head image\",\n",
    "            name_maps=name_maps,\n",
    "            citations=[fsl_cite])\n",
    "\n",
    "        pipeline.add(\n",
    "            'bet',\n",
    "            fsl.BET(\n",
    "                mask=True),\n",
    "            inputs={\n",
    "                'in_file': ('magnitude', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'brain': ('out_file', nifti_gz_format),\n",
    "                'brain_mask': ('mask_file', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        return pipeline\n",
    "\n",
    "    def smooth_mask_pipeline(self, **name_maps):\n",
    "\n",
    "        pipeline = self.new_pipeline(\n",
    "            'smooth_mask',\n",
    "            desc=\"Smooths and masks a brain image\",\n",
    "            name_maps=name_maps,\n",
    "            citations=[fsl_cite])\n",
    "\n",
    "        # Smoothing process\n",
    "        smooth = pipeline.add(\n",
    "            'smooth',\n",
    "            fsl.IsotropicSmooth(\n",
    "                fwhm=self.parameter('smoothing_fwhm')),\n",
    "            inputs={\n",
    "                'in_file': ('magnitude', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'smooth': ('out_file', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        pipeline.add(\n",
    "            'mask',\n",
    "            fsl.ApplyMask(),\n",
    "            inputs={\n",
    "                'in_file': (smooth, 'out_file'),\n",
    "                'mask_file': ('brain_mask', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'smooth_masked': ('out_file', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        return pipeline\n",
    "\n",
    "    def plot_comparision(self, figsize=(12, 4)):\n",
    "\n",
    "        for subj_i in self.subject_ids:\n",
    "            for visit_i in self.visit_ids:\n",
    "                f = plt.figure(figsize=figsize)\n",
    "                f.suptitle('Subject \"{}\" - Visit \"{}\"'.format(subj_i, visit_i))\n",
    "                for i, spec_name in enumerate(['magnitude', 'smooth',\n",
    "                                               'brain_mask', 'smooth_masked']):\n",
    "                    f.add_subplot(1, 4, i + 1)\n",
    "                    self._plot_slice(spec_name, subj_i, visit_i)\n",
    "                    plt.title(spec_name)\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_slice(self, spec_name, subject_id=None, visit_id=None):\n",
    "        # Load the image\n",
    "        data = self.data(spec_name, derive=True).item(\n",
    "            subject_id=subject_id, visit_id=visit_id).get_array()\n",
    "\n",
    "        # Cut in the middle of the brain\n",
    "        cut = int(data.shape[-1] / 2) + 10\n",
    "\n",
    "        # Plot the data\n",
    "        plt.imshow(np.rot90(data[..., cut]), cmap=\"gray\")\n",
    "        plt.gca().set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the `_plot_slice` method access the derived data using the `Analysis.data` method like we did in the \"Applying Analysis Class\" notebook. From the `FilesetSlice` it returns you can access a single data \"item\" using the `item` method. In Banana, the data array of `Fileset`s in standard image format can be accessed using the `get_arrray` method, which we then plot with Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Extend the `example.analaysis.BasicBrainAnalysis` class to add the `image_std` data-spec using the `nipype.interfaces.fsl.ImageStats` interface, which is the standard deviation of the smooth-masked image. Then run this analysis on the Tw-weighted images in the 'output/sample-datasets/depth1' dataset created in the \"Applying Analysis Classes\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "## Write your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "! fslstats -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from arcana import OutputFieldSpec\n",
    "from example.analysis import BasicBrainAnalysis\n",
    "from banana.requirement import fsl_req\n",
    "from banana.citation import fsl_cite\n",
    "\n",
    "\n",
    "class MyExtendedBasicBrainAnalysis(BasicBrainAnalysis, metaclass=AnalysisMetaClass):\n",
    "\n",
    "    add_data_specs = [\n",
    "        OutputFilesetSpec('smooth', mrtrix_image_format, 'smooth_mask_pipeline',\n",
    "                          desc=\"Smoothed magnitude image in Mrtrix format\"),\n",
    "        OutputFieldSpec('image_std', float,\n",
    "                        'image_std_pipeline',\n",
    "                        desc=\"Standard deviation of the smoothed masked image\")]\n",
    "    \n",
    "    def image_std_pipeline(self, **name_maps):\n",
    "        \n",
    "        pipeline = self.new_pipeline(\n",
    "            'image_std_pipeline',\n",
    "            desc=\"Calculates the standard deviation of the smooth masked image\",\n",
    "            name_maps=name_maps,\n",
    "            citations=[fsl_cite])\n",
    "\n",
    "        pipeline.add(\n",
    "            'mask',\n",
    "            fsl.ImageStats(\n",
    "                op_string='-s'),\n",
    "            inputs={\n",
    "                'in_file': ('smooth_masked', nifti_gz_format)},\n",
    "            outputs={\n",
    "                'image_std': ('out_stat', nifti_gz_format)},\n",
    "            requirements=[\n",
    "                fsl_req.v('5.0.10')])\n",
    "\n",
    "        return pipeline\n",
    "        \n",
    "print(MyExtendedBasicBrainAnalysis.static_menu(full=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from arcana import Dataset, FilesetFilter\n",
    "\n",
    "\n",
    "my_analysis = MyExtendedBasicBrainAnalysis(\n",
    "    'my_extended_analysis',  # The name needs to be the same as the previous version\n",
    "    dataset=Dataset('output/sample-datasets/depth1', depth=1),\n",
    "    processor='work',\n",
    "    inputs=[\n",
    "        FilesetFilter('magnitude', '.*T1w$', is_regex=True)])\n",
    "\n",
    "my_analysis.derive('image_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "for std in my_analysis.data('image_std'):\n",
    "    print('Subject/visit ({}/{}): {} '.format(std.subject_id, std.visit_id, std.value))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
